{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0888c38-b8f7-4f58-9b27-41eb44586c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 09:34:48,385 - modelscope - WARNING - Model revision not specified, use revision: v1.0.8\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 8.21k/8.21k [00:00<00:00, 11.7MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 50.8k/50.8k [00:00<00:00, 689kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1.17k/1.17k [00:00<00:00, 3.86MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 76.0/76.0 [00:00<00:00, 287kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2.29k/2.29k [00:00<00:00, 9.92MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1.88k/1.88k [00:00<00:00, 7.83MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 249/249 [00:00<00:00, 890kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 6.73k/6.73k [00:00<00:00, 18.1MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████▉| 1.91G/1.91G [00:19<00:00, 105MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████▉| 1.89G/1.89G [00:31<00:00, 63.9MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████▉| 1.90G/1.90G [00:21<00:00, 96.8MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████▉| 1.87G/1.87G [00:27<00:00, 73.1MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████▉| 1.45G/1.45G [00:17<00:00, 89.8MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 80.2k/80.2k [00:00<00:00, 1.38MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 54.5k/54.5k [00:00<00:00, 934kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2.64k/2.64k [00:00<00:00, 11.7MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [00:00<00:00, 960kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2.44M/2.44M [00:00<00:00, 11.0MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 14.3k/14.3k [00:00<00:00, 481kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 27.2k/27.2k [00:00<00:00, 841kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 9.39k/9.39k [00:00<00:00, 14.7MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 173/173 [00:00<00:00, 766kB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "# Downloading model checkpoint to a local dir model_dir\n",
    "model_dir = snapshot_download('Qwen/Qwen-14B-Chat-Int4', cache_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8d729-4361-45de-abad-1f8699fe148c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b0f7c0b-c954-4d20-8562-c225e63d77c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-09 09:44:40 config.py:211] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 04-09 09:44:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='Qwen/Qwen1___5-14B-Chat-AWQ/', tokenizer='Qwen/Qwen1___5-14B-Chat-AWQ/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 09:44:43 model_runner.py:104] Loading model weights took 9.0594 GB\n",
      "INFO 04-09 09:44:48 gpu_executor.py:94] # GPU blocks: 839, # CPU blocks: 327\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model's max seq len (32768) is larger than the maximum number of tokens that can be stored in KV cache (13424). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[1;32m      2\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, my name is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe president of the United States is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe capital of France is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe future of AI is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen1___5-14B-Chat-AWQ/\u001b[39m\u001b[38;5;124m\"\u001b[39m, gpu_memory_utilization\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/llm.py:112\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     94\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     95\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     96\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    111\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m LLMEngine\u001b[38;5;241m.\u001b[39mfrom_engine_args(\n\u001b[1;32m    113\u001b[0m     engine_args, usage_context\u001b[38;5;241m=\u001b[39mUsageContext\u001b[38;5;241m.\u001b[39mLLM_CLASS)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/llm_engine.py:196\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    193\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;241m*\u001b[39mengine_configs,\n\u001b[1;32m    198\u001b[0m     executor_class\u001b[38;5;241m=\u001b[39mexecutor_class,\n\u001b[1;32m    199\u001b[0m     log_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m engine_args\u001b[38;5;241m.\u001b[39mdisable_log_stats,\n\u001b[1;32m    200\u001b[0m     usage_context\u001b[38;5;241m=\u001b[39musage_context,\n\u001b[1;32m    201\u001b[0m )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/llm_engine.py:110\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config, executor_class, log_stats, usage_context)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenizer \u001b[38;5;241m=\u001b[39m Detokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(model_config, cache_config,\n\u001b[1;32m    111\u001b[0m                                      parallel_config, scheduler_config,\n\u001b[1;32m    112\u001b[0m                                      device_config, lora_config,\n\u001b[1;32m    113\u001b[0m                                      vision_language_config)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:40\u001b[0m, in \u001b[0;36mGPUExecutor.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_worker()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_cache()\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:97\u001b[0m, in \u001b[0;36mGPUExecutor._init_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m     num_gpu_blocks \u001b[38;5;241m=\u001b[39m forced_num_gpu_blocks\n\u001b[1;32m     94\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# GPU blocks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_gpu_blocks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# CPU blocks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cpu_blocks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m check_block_size_valid(num_gpu_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mblock_size,\n\u001b[1;32m     98\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mmax_model_len)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/executor/utils.py:8\u001b[0m, in \u001b[0;36mcheck_block_size_valid\u001b[0;34m(num_gpu_blocks, block_size, max_model_len)\u001b[0m\n\u001b[1;32m      6\u001b[0m max_seq_len \u001b[38;5;241m=\u001b[39m block_size \u001b[38;5;241m*\u001b[39m num_gpu_blocks\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_model_len \u001b[38;5;241m>\u001b[39m max_seq_len:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms max seq len (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_model_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis larger than the maximum number of tokens that can be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstored in KV cache (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_seq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Try increasing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`gpu_memory_utilization` or decreasing `max_model_len` when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitializing the engine.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The model's max seq len (32768) is larger than the maximum number of tokens that can be stored in KV cache (13424). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine."
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "llm = LLM(model=\"Qwen/Qwen1___5-14B-Chat-AWQ/\", gpu_memory_utilization=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "876aaea4-796a-4903-ac52-ff9baf18a807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wang Lei and I am from China. I am a 14-year-old\n",
      " the head of state and government of the United States. The president leads the executive\n",
      "____．（　　）\n",
      "A. Berlin\n",
      "B. Rome\n",
      "C.\n",
      " exciting， but it also raises important questions about the impact on jobs and society.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b900e0-5f64-4f1f-883a-8a5d56fc7fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-08 15:20:30 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='Qwen/Qwen1___5-7B-Chat/', tokenizer='Qwen/Qwen1___5-7B-Chat/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-08 15:20:32 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance.\n",
      "INFO 04-08 15:20:32 selector.py:25] Using XFormers backend.\n",
      "INFO 04-08 15:20:39 model_runner.py:104] Loading model weights took 14.3919 GB\n",
      "INFO 04-08 15:20:42 gpu_executor.py:94] # GPU blocks: 2264, # CPU blocks: 512\n",
      "INFO 04-08 15:20:44 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-08 15:20:44 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-08 15:20:49 model_runner.py:867] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "import os\n",
    "\n",
    "llm = VLLM(\n",
    "    model=\"Qwen/Qwen1___5-7B-Chat/\",\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=2560,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58a1e022-7b8d-446c-b4e7-29d2b90c90cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/liucd/langchain\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e60894-5141-44aa-96bd-705115a8336b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "417fc84a-f0e5-4a45-8cd3-b884d0c2ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-22 15:10:42 api_server.py:151] vLLM API server version 0.4.1\n",
      "INFO 05-22 15:10:42 api_server.py:152] args: Namespace(host=None, port=1122, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=['Qwen1.5-32B-Chat-GPTQ'], lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='Qwen/Qwen-72B-Chat-Int4/', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='outlines', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.8, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization='gptq', enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_max_model_len=None, model_loader_extra_config=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\n",
      "WARNING 05-22 15:10:42 config.py:169] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "2024-05-22 15:10:44,818\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "INFO 05-22 15:10:46 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='Qwen/Qwen-72B-Chat-Int4/', speculative_config=None, tokenizer='Qwen/Qwen-72B-Chat-Int4/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n",
      "WARNING 05-22 15:10:47 tokenizer.py:123] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 05-22 15:10:51 utils.py:608] Found nccl from library /home/liuchangdong/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2724619)\u001b[0m INFO 05-22 15:10:51 utils.py:608] Found nccl from library /home/liuchangdong/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-22 15:10:53 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-22 15:10:53 selector.py:33] Using XFormers backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=2724619)\u001b[0m INFO 05-22 15:10:56 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "\u001b[36m(RayWorkerWrapper pid=2724619)\u001b[0m INFO 05-22 15:10:56 selector.py:33] Using XFormers backend.\n",
      "INFO 05-22 15:10:58 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=2724619)\u001b[0m INFO 05-22 15:10:58 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 05-22 15:11:04 utils.py:129] reading GPU P2P access cache from /home/liuchangdong/.config/vllm/gpu_p2p_access_cache_for_1,5.json\n",
      "\u001b[36m(RayWorkerWrapper pid=2724619)\u001b[0m INFO 05-22 15:11:04 utils.py:129] reading GPU P2P access cache from /home/liuchangdong/.config/vllm/gpu_p2p_access_cache_for_1,5.json\n",
      "INFO 05-22 15:11:16 model_runner.py:173] Loading model weights took 19.2528 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=2724619)\u001b[0m INFO 05-22 15:11:23 model_runner.py:173] Loading model weights took 19.2528 GB\n",
      "INFO 05-22 15:11:28 ray_gpu_executor.py:217] # GPU blocks: 523, # CPU blocks: 204\n",
      "INFO 05-22 15:11:36 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-22 15:11:36 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=2724619)\u001b[0m INFO 05-22 15:11:36 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=2724619)\u001b[0m INFO 05-22 15:11:36 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-22 15:12:06 custom_all_reduce.py:246] Registering 5635 cuda graph addresses\n",
      "INFO 05-22 15:12:06 model_runner.py:1057] Graph capturing finished in 30 secs.\n",
      "\u001b[36m(RayWorkerWrapper pid=2724619)\u001b[0m INFO 05-22 15:12:06 custom_all_reduce.py:246] Registering 5635 cuda graph addresses\n",
      "\u001b[36m(RayWorkerWrapper pid=2724619)\u001b[0m INFO 05-22 15:12:06 model_runner.py:1057] Graph capturing finished in 30 secs.\n",
      "WARNING 05-22 15:12:06 tokenizer.py:123] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "WARNING 05-22 15:12:06 serving_chat.py:347] No chat template provided. Chat API will not work.\n",
      "WARNING 05-22 15:12:07 tokenizer.py:123] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m2719970\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:1122\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 05-22 15:12:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:12:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:12:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:12:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "INFO 05-22 15:12:47 async_llm_engine.py:524] Received request cmpl-ef3bc2b5c6d24c6cad15ba71e6f62e61: prompt: '<|im_start|>user\\n你是一位资深医学专家，目前需要从报告\\'检查所见\\'中的关于肺结节的核心字段，主要包括\\'原文句子\\',\\'结节部位\\',\\'结节大小\\',\\'结节类型\\',\\'结节边界\\', \\'淋巴结情况\\', 需要从\\'检查诊断\\'中的信息用于提取“LUNG-RADs”字段，然后输出一个json格便于后续的处理, 返回形式类似 list of dict 格式:\\n\\n#要求：\\n1、请输出一个合法的 JSON 字符串, 参考 sample output 的输出格式;\\n2、“检查所见”字段信息需要抽取对应原文，找到肺结节相关信息;\\n3、“检查所见”字段信息不需要抽取原文，需要协助找到\"LUNG-RADs\"的值，只有“结节类型”为肺结节类型时，才能有值，否则为空\"-\";\\n\\n\\n#example:\\n#sample input：\\n检查所见: 左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断: 对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#sample output:\\n[\\n    {\\n        \\'原文句子\\': \\'左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚\\',\\n        \\'结节部位\\': \\'左肺上、下叶\\',\\n        \\'结节大小\\': \\'-\\',\\n        \\'结节类型\\': \\'团片状、斑点、结节高密度影,钙化\\',\\n        \\'结节边界\\': \\'边界不清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n    {\\n        \\'原文句子\\': \\'另两肺见多发小结节影，直径约3-4mm，界清\\',\\n        \\'结节部位\\': \\'两肺\\',\\n        \\'结节大小\\': \\'直径约3-4mm\\',\\n        \\'结节类型\\': \\'小结节影\\',\\n        \\'结节边界\\': \\'界清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n]\\n\\n#input:\\n检查所见:左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断:对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#output:\\n\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=3093, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 872, 198, 56568, 109182, 107505, 104316, 101057, 3837, 100004, 85106, 45181, 100370, 6, 101071, 31838, 88970, 6, 101047, 101888, 100887, 36885, 55502, 104867, 44931, 3837, 110800, 6, 103283, 109949, 1844, 36885, 55502, 105130, 1844, 36885, 55502, 92032, 1844, 36885, 55502, 31905, 1844, 36885, 55502, 108758, 516, 364, 114814, 36885, 99559, 516, 18137, 250, 222, 30534, 45181, 6, 101071, 105262, 6, 101047, 27369, 100751, 107439, 2073, 43, 60002, 10911, 1808, 82, 854, 44931, 3837, 101889, 66017, 46944, 2236, 33983, 108085, 105463, 9370, 54542, 11, 55616, 100414, 101412, 1140, 315, 6451, 51461, 120, 28330, 1447, 2, 101882, 28311, 16, 5373, 14880, 66017, 46944, 101431, 9370, 4718, 73312, 38304, 51575, 11, 26853, 224, 77598, 6077, 2550, 43589, 66017, 68805, 280, 17, 91956, 101071, 31838, 88970, 854, 44931, 27369, 85106, 118797, 103124, 103283, 3837, 101958, 100887, 36885, 55502, 110984, 280, 18, 91956, 101071, 31838, 88970, 854, 44931, 27369, 104689, 118797, 103283, 3837, 85106, 105096, 101958, 73630, 60002, 10911, 1808, 82, 1, 9370, 25511, 3837, 101043, 2073, 36885, 55502, 31905, 854, 17714, 100887, 36885, 55502, 31905, 13343, 3837, 101901, 18830, 25511, 3837, 104420, 50647, 34294, 29122, 2, 8687, 510, 2, 13611, 1946, 28311, 101071, 31838, 88970, 25, 83002, 99, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 69162, 56006, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 13611, 2550, 510, 9640, 262, 341, 286, 364, 103283, 109949, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 751, 286, 364, 36885, 55502, 92032, 1210, 7788, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 11, 106033, 32108, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 108758, 106122, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 262, 341, 286, 364, 103283, 109949, 1210, 364, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77540, 100887, 751, 286, 364, 36885, 55502, 92032, 1210, 364, 109808, 94237, 18, 12, 19, 3821, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 30709, 36885, 55502, 57222, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 97120, 79766, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 2533, 2, 1355, 510, 101071, 31838, 88970, 25, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 104877, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 3006, 1447, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-22 15:12:52 metrics.py:229] Avg prompt throughput: 200.5 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:12:57 async_llm_engine.py:120] Finished request cmpl-ef3bc2b5c6d24c6cad15ba71e6f62e61.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38608 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-22 15:13:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:13:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:13:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:13:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:13:42 async_llm_engine.py:524] Received request cmpl-84f985b6a462492baf5861b67aef6591: prompt: '<|im_start|>user\\n你是一位资深医学专家，目前需要从报告\\'检查所见\\'中的关于肺结节的核心字段，主要包括\\'原文句子\\',\\'结节部位\\',\\'结节大小\\',\\'结节类型\\',\\'结节边界\\', \\'淋巴结情况\\', 需要从\\'检查诊断\\'中的信息用于提取“LUNG-RADs”字段，然后输出一个json格便于后续的处理, 返回形式类似 list of dict 格式:\\n\\n#要求：\\n1、请输出一个合法的 JSON 字符串, 参考 sample output 的输出格式;\\n2、“检查所见”字段信息需要抽取对应原文，找到肺结节相关信息;\\n3、“检查所见”字段信息不需要抽取原文，需要协助找到\"LUNG-RADs\"的值，只有“结节类型”为肺结节类型时，才能有值，否则为空\"-\";\\n\\n\\n#example:\\n#sample input：\\n检查所见: 左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断: 对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#sample output:\\n[\\n    {\\n        \\'原文句子\\': \\'左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚\\',\\n        \\'结节部位\\': \\'左肺上、下叶\\',\\n        \\'结节大小\\': \\'-\\',\\n        \\'结节类型\\': \\'团片状、斑点、结节高密度影,钙化\\',\\n        \\'结节边界\\': \\'边界不清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n    {\\n        \\'原文句子\\': \\'另两肺见多发小结节影，直径约3-4mm，界清\\',\\n        \\'结节部位\\': \\'两肺\\',\\n        \\'结节大小\\': \\'直径约3-4mm\\',\\n        \\'结节类型\\': \\'小结节影\\',\\n        \\'结节边界\\': \\'界清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n]\\n\\n#input:\\n检查所见:左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断:对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#output:\\n\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=3093, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 872, 198, 56568, 109182, 107505, 104316, 101057, 3837, 100004, 85106, 45181, 100370, 6, 101071, 31838, 88970, 6, 101047, 101888, 100887, 36885, 55502, 104867, 44931, 3837, 110800, 6, 103283, 109949, 1844, 36885, 55502, 105130, 1844, 36885, 55502, 92032, 1844, 36885, 55502, 31905, 1844, 36885, 55502, 108758, 516, 364, 114814, 36885, 99559, 516, 18137, 250, 222, 30534, 45181, 6, 101071, 105262, 6, 101047, 27369, 100751, 107439, 2073, 43, 60002, 10911, 1808, 82, 854, 44931, 3837, 101889, 66017, 46944, 2236, 33983, 108085, 105463, 9370, 54542, 11, 55616, 100414, 101412, 1140, 315, 6451, 51461, 120, 28330, 1447, 2, 101882, 28311, 16, 5373, 14880, 66017, 46944, 101431, 9370, 4718, 73312, 38304, 51575, 11, 26853, 224, 77598, 6077, 2550, 43589, 66017, 68805, 280, 17, 91956, 101071, 31838, 88970, 854, 44931, 27369, 85106, 118797, 103124, 103283, 3837, 101958, 100887, 36885, 55502, 110984, 280, 18, 91956, 101071, 31838, 88970, 854, 44931, 27369, 104689, 118797, 103283, 3837, 85106, 105096, 101958, 73630, 60002, 10911, 1808, 82, 1, 9370, 25511, 3837, 101043, 2073, 36885, 55502, 31905, 854, 17714, 100887, 36885, 55502, 31905, 13343, 3837, 101901, 18830, 25511, 3837, 104420, 50647, 34294, 29122, 2, 8687, 510, 2, 13611, 1946, 28311, 101071, 31838, 88970, 25, 83002, 99, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 69162, 56006, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 13611, 2550, 510, 9640, 262, 341, 286, 364, 103283, 109949, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 751, 286, 364, 36885, 55502, 92032, 1210, 7788, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 11, 106033, 32108, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 108758, 106122, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 262, 341, 286, 364, 103283, 109949, 1210, 364, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77540, 100887, 751, 286, 364, 36885, 55502, 92032, 1210, 364, 109808, 94237, 18, 12, 19, 3821, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 30709, 36885, 55502, 57222, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 97120, 79766, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 2533, 2, 1355, 510, 101071, 31838, 88970, 25, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 104877, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 3006, 1447, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-22 15:13:43 metrics.py:229] Avg prompt throughput: 158.5 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:13:48 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 34.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.1%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:13:50 async_llm_engine.py:120] Finished request cmpl-84f985b6a462492baf5861b67aef6591.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:43872 - \"\u001b[1mPOST /v1/chat/completions?response_format%5Btype%5D=json_object HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-22 15:13:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:14:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:14:07 async_llm_engine.py:524] Received request cmpl-5ad03087c7b543be8232f3eae30f89e5: prompt: '<|im_start|>user\\n你是一位资深医学专家，目前需要从报告\\'检查所见\\'中的关于肺结节的核心字段，主要包括\\'原文句子\\',\\'结节部位\\',\\'结节大小\\',\\'结节类型\\',\\'结节边界\\', \\'淋巴结情况\\', 需要从\\'检查诊断\\'中的信息用于提取“LUNG-RADs”字段，然后输出一个json格便于后续的处理, 返回形式类似 list of dict 格式:\\n\\n#要求：\\n1、请输出一个合法的 JSON 字符串, 参考 sample output 的输出格式;\\n2、“检查所见”字段信息需要抽取对应原文，找到肺结节相关信息;\\n3、“检查所见”字段信息不需要抽取原文，需要协助找到\"LUNG-RADs\"的值，只有“结节类型”为肺结节类型时，才能有值，否则为空\"-\";\\n\\n\\n#example:\\n#sample input：\\n检查所见: 左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断: 对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#sample output:\\n[\\n    {\\n        \\'原文句子\\': \\'左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚\\',\\n        \\'结节部位\\': \\'左肺上、下叶\\',\\n        \\'结节大小\\': \\'-\\',\\n        \\'结节类型\\': \\'团片状、斑点、结节高密度影,钙化\\',\\n        \\'结节边界\\': \\'边界不清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n    {\\n        \\'原文句子\\': \\'另两肺见多发小结节影，直径约3-4mm，界清\\',\\n        \\'结节部位\\': \\'两肺\\',\\n        \\'结节大小\\': \\'直径约3-4mm\\',\\n        \\'结节类型\\': \\'小结节影\\',\\n        \\'结节边界\\': \\'界清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n]\\n\\n#input:\\n检查所见:左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断:对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#output:\\n\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=3093, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 872, 198, 56568, 109182, 107505, 104316, 101057, 3837, 100004, 85106, 45181, 100370, 6, 101071, 31838, 88970, 6, 101047, 101888, 100887, 36885, 55502, 104867, 44931, 3837, 110800, 6, 103283, 109949, 1844, 36885, 55502, 105130, 1844, 36885, 55502, 92032, 1844, 36885, 55502, 31905, 1844, 36885, 55502, 108758, 516, 364, 114814, 36885, 99559, 516, 18137, 250, 222, 30534, 45181, 6, 101071, 105262, 6, 101047, 27369, 100751, 107439, 2073, 43, 60002, 10911, 1808, 82, 854, 44931, 3837, 101889, 66017, 46944, 2236, 33983, 108085, 105463, 9370, 54542, 11, 55616, 100414, 101412, 1140, 315, 6451, 51461, 120, 28330, 1447, 2, 101882, 28311, 16, 5373, 14880, 66017, 46944, 101431, 9370, 4718, 73312, 38304, 51575, 11, 26853, 224, 77598, 6077, 2550, 43589, 66017, 68805, 280, 17, 91956, 101071, 31838, 88970, 854, 44931, 27369, 85106, 118797, 103124, 103283, 3837, 101958, 100887, 36885, 55502, 110984, 280, 18, 91956, 101071, 31838, 88970, 854, 44931, 27369, 104689, 118797, 103283, 3837, 85106, 105096, 101958, 73630, 60002, 10911, 1808, 82, 1, 9370, 25511, 3837, 101043, 2073, 36885, 55502, 31905, 854, 17714, 100887, 36885, 55502, 31905, 13343, 3837, 101901, 18830, 25511, 3837, 104420, 50647, 34294, 29122, 2, 8687, 510, 2, 13611, 1946, 28311, 101071, 31838, 88970, 25, 83002, 99, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 69162, 56006, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 13611, 2550, 510, 9640, 262, 341, 286, 364, 103283, 109949, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 751, 286, 364, 36885, 55502, 92032, 1210, 7788, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 11, 106033, 32108, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 108758, 106122, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 262, 341, 286, 364, 103283, 109949, 1210, 364, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77540, 100887, 751, 286, 364, 36885, 55502, 92032, 1210, 364, 109808, 94237, 18, 12, 19, 3821, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 30709, 36885, 55502, 57222, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 97120, 79766, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 2533, 2, 1355, 510, 101071, 31838, 88970, 25, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 104877, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 3006, 1447, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-22 15:14:08 async_llm_engine.py:154] Aborted request cmpl-5ad03087c7b543be8232f3eae30f89e5.\n",
      "INFO 05-22 15:14:17 metrics.py:229] Avg prompt throughput: 100.3 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:14:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:14:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:14:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:14:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:15:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:15:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:15:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:15:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:15:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:15:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:16:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:16:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:16:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:16:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:16:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:16:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:17:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:17:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:17:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:17:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:17:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:17:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:18:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:18:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:18:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:18:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:18:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:18:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:19:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:19:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:19:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:19:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:19:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:19:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:20:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:20:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:20:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:20:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:20:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:20:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:21:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:21:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:21:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:21:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:21:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:21:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:22:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:22:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:22:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:22:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:22:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:22:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:23:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:23:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:23:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:23:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:23:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:23:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:24:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:24:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:24:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:24:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:24:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:24:56 async_llm_engine.py:524] Received request cmpl-83a8871af5d247a4a7d567b086448b85: prompt: '<|im_start|>user\\n你是一位资深医学专家，目前需要从报告\\'检查所见\\'中的关于肺结节的核心字段，主要包括\\'原文句子\\',\\'结节部位\\',\\'结节大小\\',\\'结节类型\\',\\'结节边界\\', \\'淋巴结情况\\', 需要从\\'检查诊断\\'中的信息用于提取“LUNG-RADs”字段，然后输出一个json格便于后续的处理, 返回形式类似 list of dict 格式:\\n\\n#要求：\\n1、请输出一个合法的 JSON 字符串, 参考 sample output 的输出格式;\\n2、“检查所见”字段信息需要抽取对应原文，找到肺结节相关信息;\\n3、“检查所见”字段信息不需要抽取原文，需要协助找到\"LUNG-RADs\"的值，只有“结节类型”为肺结节类型时，才能有值，否则为空\"-\";\\n\\n\\n#example:\\n#sample input：\\n检查所见: 左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断: 对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#sample output:\\n[\\n    {\\n        \\'原文句子\\': \\'左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚\\',\\n        \\'结节部位\\': \\'左肺上、下叶\\',\\n        \\'结节大小\\': \\'-\\',\\n        \\'结节类型\\': \\'团片状、斑点、结节高密度影,钙化\\',\\n        \\'结节边界\\': \\'边界不清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n    {\\n        \\'原文句子\\': \\'另两肺见多发小结节影，直径约3-4mm，界清\\',\\n        \\'结节部位\\': \\'两肺\\',\\n        \\'结节大小\\': \\'直径约3-4mm\\',\\n        \\'结节类型\\': \\'小结节影\\',\\n        \\'结节边界\\': \\'界清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n]\\n\\n#input:\\n检查所见:左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断:对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#output:\\n\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=3093, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 872, 198, 56568, 109182, 107505, 104316, 101057, 3837, 100004, 85106, 45181, 100370, 6, 101071, 31838, 88970, 6, 101047, 101888, 100887, 36885, 55502, 104867, 44931, 3837, 110800, 6, 103283, 109949, 1844, 36885, 55502, 105130, 1844, 36885, 55502, 92032, 1844, 36885, 55502, 31905, 1844, 36885, 55502, 108758, 516, 364, 114814, 36885, 99559, 516, 18137, 250, 222, 30534, 45181, 6, 101071, 105262, 6, 101047, 27369, 100751, 107439, 2073, 43, 60002, 10911, 1808, 82, 854, 44931, 3837, 101889, 66017, 46944, 2236, 33983, 108085, 105463, 9370, 54542, 11, 55616, 100414, 101412, 1140, 315, 6451, 51461, 120, 28330, 1447, 2, 101882, 28311, 16, 5373, 14880, 66017, 46944, 101431, 9370, 4718, 73312, 38304, 51575, 11, 26853, 224, 77598, 6077, 2550, 43589, 66017, 68805, 280, 17, 91956, 101071, 31838, 88970, 854, 44931, 27369, 85106, 118797, 103124, 103283, 3837, 101958, 100887, 36885, 55502, 110984, 280, 18, 91956, 101071, 31838, 88970, 854, 44931, 27369, 104689, 118797, 103283, 3837, 85106, 105096, 101958, 73630, 60002, 10911, 1808, 82, 1, 9370, 25511, 3837, 101043, 2073, 36885, 55502, 31905, 854, 17714, 100887, 36885, 55502, 31905, 13343, 3837, 101901, 18830, 25511, 3837, 104420, 50647, 34294, 29122, 2, 8687, 510, 2, 13611, 1946, 28311, 101071, 31838, 88970, 25, 83002, 99, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 69162, 56006, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 13611, 2550, 510, 9640, 262, 341, 286, 364, 103283, 109949, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 751, 286, 364, 36885, 55502, 92032, 1210, 7788, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 11, 106033, 32108, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 108758, 106122, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 262, 341, 286, 364, 103283, 109949, 1210, 364, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77540, 100887, 751, 286, 364, 36885, 55502, 92032, 1210, 364, 109808, 94237, 18, 12, 19, 3821, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 30709, 36885, 55502, 57222, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 97120, 79766, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 2533, 2, 1355, 510, 101071, 31838, 88970, 25, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 104877, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 3006, 1447, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-22 15:24:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:25:02 metrics.py:229] Avg prompt throughput: 200.4 tokens/s, Avg generation throughput: 36.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.1%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:25:04 async_llm_engine.py:120] Finished request cmpl-83a8871af5d247a4a7d567b086448b85.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:36662 - \"\u001b[1mPOST /v1/chat/completions?response_format%5Btype%5D=json_object&guided_json%5Btype%5D=object&guided_json%5Bproperties%5D%5Bfirst_key%5D%5Btype%5D=array&guided_json%5Bproperties%5D%5Bfirst_key%5D%5Bitems%5D%5Btype%5D=string&guided_json%5Bproperties%5D%5Bfirst_key%5D%5BminItems%5D=1&guided_json%5Bproperties%5D%5Banother_key%5D%5Btype%5D=array&guided_json%5Bproperties%5D%5Banother_key%5D%5Bitems%5D%5Btype%5D=string&guided_json%5Bproperties%5D%5Banother_key%5D%5BminItems%5D=1&guided_json%5Bproperties%5D%5Brequired%5D=first_key%2Canother_key HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-22 15:25:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:25:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:25:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:25:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:25:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:26:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:26:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:26:23 async_llm_engine.py:524] Received request cmpl-5271d900299b432f8eb2dfb40e338ba7: prompt: '<|im_start|>user\\n你是一位资深医学专家，目前需要从报告\\'检查所见\\'中的关于肺结节的核心字段，主要包括\\'原文句子\\',\\'结节部位\\',\\'结节大小\\',\\'结节类型\\',\\'结节边界\\', \\'淋巴结情况\\', 需要从\\'检查诊断\\'中的信息用于提取“LUNG-RADs”字段，然后输出一个json格便于后续的处理, 返回形式类似 list of dict 格式:\\n\\n#要求：\\n1、请输出一个合法的 JSON 字符串, 参考 sample output 的输出格式;\\n2、“检查所见”字段信息需要抽取对应原文，找到肺结节相关信息;\\n3、“检查所见”字段信息不需要抽取原文，需要协助找到\"LUNG-RADs\"的值，只有“结节类型”为肺结节类型时，才能有值，否则为空\"-\";\\n\\n\\n#example:\\n#sample input：\\n检查所见: 左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断: 对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#sample output:\\n[\\n    {\\n        \\'原文句子\\': \\'左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚\\',\\n        \\'结节部位\\': \\'左肺上、下叶\\',\\n        \\'结节大小\\': \\'-\\',\\n        \\'结节类型\\': \\'团片状、斑点、结节高密度影,钙化\\',\\n        \\'结节边界\\': \\'边界不清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n    {\\n        \\'原文句子\\': \\'另两肺见多发小结节影，直径约3-4mm，界清\\',\\n        \\'结节部位\\': \\'两肺\\',\\n        \\'结节大小\\': \\'直径约3-4mm\\',\\n        \\'结节类型\\': \\'小结节影\\',\\n        \\'结节边界\\': \\'界清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n]\\n\\n#input:\\n检查所见:左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断:对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#output:\\n\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=3093, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 872, 198, 56568, 109182, 107505, 104316, 101057, 3837, 100004, 85106, 45181, 100370, 6, 101071, 31838, 88970, 6, 101047, 101888, 100887, 36885, 55502, 104867, 44931, 3837, 110800, 6, 103283, 109949, 1844, 36885, 55502, 105130, 1844, 36885, 55502, 92032, 1844, 36885, 55502, 31905, 1844, 36885, 55502, 108758, 516, 364, 114814, 36885, 99559, 516, 18137, 250, 222, 30534, 45181, 6, 101071, 105262, 6, 101047, 27369, 100751, 107439, 2073, 43, 60002, 10911, 1808, 82, 854, 44931, 3837, 101889, 66017, 46944, 2236, 33983, 108085, 105463, 9370, 54542, 11, 55616, 100414, 101412, 1140, 315, 6451, 51461, 120, 28330, 1447, 2, 101882, 28311, 16, 5373, 14880, 66017, 46944, 101431, 9370, 4718, 73312, 38304, 51575, 11, 26853, 224, 77598, 6077, 2550, 43589, 66017, 68805, 280, 17, 91956, 101071, 31838, 88970, 854, 44931, 27369, 85106, 118797, 103124, 103283, 3837, 101958, 100887, 36885, 55502, 110984, 280, 18, 91956, 101071, 31838, 88970, 854, 44931, 27369, 104689, 118797, 103283, 3837, 85106, 105096, 101958, 73630, 60002, 10911, 1808, 82, 1, 9370, 25511, 3837, 101043, 2073, 36885, 55502, 31905, 854, 17714, 100887, 36885, 55502, 31905, 13343, 3837, 101901, 18830, 25511, 3837, 104420, 50647, 34294, 29122, 2, 8687, 510, 2, 13611, 1946, 28311, 101071, 31838, 88970, 25, 83002, 99, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 69162, 56006, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 13611, 2550, 510, 9640, 262, 341, 286, 364, 103283, 109949, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 751, 286, 364, 36885, 55502, 92032, 1210, 7788, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 11, 106033, 32108, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 108758, 106122, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 262, 341, 286, 364, 103283, 109949, 1210, 364, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77540, 100887, 751, 286, 364, 36885, 55502, 92032, 1210, 364, 109808, 94237, 18, 12, 19, 3821, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 30709, 36885, 55502, 57222, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 97120, 79766, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 2533, 2, 1355, 510, 101071, 31838, 88970, 25, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 104877, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 3006, 1447, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-22 15:26:23 metrics.py:229] Avg prompt throughput: 157.0 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:26:28 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 37.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.3%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:26:30 async_llm_engine.py:120] Finished request cmpl-5271d900299b432f8eb2dfb40e338ba7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:41950 - \"\u001b[1mPOST /v1/chat/completions?response_format%5Btype%5D=jsn_object&guided_json%5Btype%5D=object&guided_json%5Bproperties%5D%5Bfirst_key%5D%5Btype%5D=array&guided_json%5Bproperties%5D%5Bfirst_key%5D%5Bitems%5D%5Btype%5D=string&guided_json%5Bproperties%5D%5Bfirst_key%5D%5BminItems%5D=1&guided_json%5Bproperties%5D%5Banother_key%5D%5Btype%5D=array&guided_json%5Bproperties%5D%5Banother_key%5D%5Bitems%5D%5Btype%5D=string&guided_json%5Bproperties%5D%5Banother_key%5D%5BminItems%5D=1&guided_json%5Bproperties%5D%5Brequired%5D=first_key%2Canother_key HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-22 15:26:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:26:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:26:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:27:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:27:15 async_llm_engine.py:524] Received request cmpl-6685887102844494b3b5c7ce9eb17ee3: prompt: '<|im_start|>user\\n你是一位资深医学专家，目前需要从报告\\'检查所见\\'中的关于肺结节的核心字段，主要包括\\'原文句子\\',\\'结节部位\\',\\'结节大小\\',\\'结节类型\\',\\'结节边界\\', \\'淋巴结情况\\', 需要从\\'检查诊断\\'中的信息用于提取“LUNG-RADs”字段，然后输出一个json格便于后续的处理, 返回形式类似 list of dict 格式:\\n\\n#要求：\\n1、请输出一个合法的 JSON 字符串, 参考 sample output 的输出格式;\\n2、“检查所见”字段信息需要抽取对应原文，找到肺结节相关信息;\\n3、“检查所见”字段信息不需要抽取原文，需要协助找到\"LUNG-RADs\"的值，只有“结节类型”为肺结节类型时，才能有值，否则为空\"-\";\\n\\n\\n#example:\\n#sample input：\\n检查所见: 左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断: 对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#sample output:\\n[\\n    {\\n        \\'原文句子\\': \\'左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚\\',\\n        \\'结节部位\\': \\'左肺上、下叶\\',\\n        \\'结节大小\\': \\'-\\',\\n        \\'结节类型\\': \\'团片状、斑点、结节高密度影,钙化\\',\\n        \\'结节边界\\': \\'边界不清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n    {\\n        \\'原文句子\\': \\'另两肺见多发小结节影，直径约3-4mm，界清\\',\\n        \\'结节部位\\': \\'两肺\\',\\n        \\'结节大小\\': \\'直径约3-4mm\\',\\n        \\'结节类型\\': \\'小结节影\\',\\n        \\'结节边界\\': \\'界清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n]\\n\\n#input:\\n检查所见:左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断:对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#output:\\n\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=3093, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 872, 198, 56568, 109182, 107505, 104316, 101057, 3837, 100004, 85106, 45181, 100370, 6, 101071, 31838, 88970, 6, 101047, 101888, 100887, 36885, 55502, 104867, 44931, 3837, 110800, 6, 103283, 109949, 1844, 36885, 55502, 105130, 1844, 36885, 55502, 92032, 1844, 36885, 55502, 31905, 1844, 36885, 55502, 108758, 516, 364, 114814, 36885, 99559, 516, 18137, 250, 222, 30534, 45181, 6, 101071, 105262, 6, 101047, 27369, 100751, 107439, 2073, 43, 60002, 10911, 1808, 82, 854, 44931, 3837, 101889, 66017, 46944, 2236, 33983, 108085, 105463, 9370, 54542, 11, 55616, 100414, 101412, 1140, 315, 6451, 51461, 120, 28330, 1447, 2, 101882, 28311, 16, 5373, 14880, 66017, 46944, 101431, 9370, 4718, 73312, 38304, 51575, 11, 26853, 224, 77598, 6077, 2550, 43589, 66017, 68805, 280, 17, 91956, 101071, 31838, 88970, 854, 44931, 27369, 85106, 118797, 103124, 103283, 3837, 101958, 100887, 36885, 55502, 110984, 280, 18, 91956, 101071, 31838, 88970, 854, 44931, 27369, 104689, 118797, 103283, 3837, 85106, 105096, 101958, 73630, 60002, 10911, 1808, 82, 1, 9370, 25511, 3837, 101043, 2073, 36885, 55502, 31905, 854, 17714, 100887, 36885, 55502, 31905, 13343, 3837, 101901, 18830, 25511, 3837, 104420, 50647, 34294, 29122, 2, 8687, 510, 2, 13611, 1946, 28311, 101071, 31838, 88970, 25, 83002, 99, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 69162, 56006, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 13611, 2550, 510, 9640, 262, 341, 286, 364, 103283, 109949, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 751, 286, 364, 36885, 55502, 92032, 1210, 7788, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 11, 106033, 32108, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 108758, 106122, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 262, 341, 286, 364, 103283, 109949, 1210, 364, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77540, 100887, 751, 286, 364, 36885, 55502, 92032, 1210, 364, 109808, 94237, 18, 12, 19, 3821, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 30709, 36885, 55502, 57222, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 97120, 79766, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 2533, 2, 1355, 510, 101071, 31838, 88970, 25, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 104877, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 3006, 1447, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-22 15:27:16 metrics.py:229] Avg prompt throughput: 108.1 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:27:21 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 37.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.3%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:27:23 async_llm_engine.py:120] Finished request cmpl-6685887102844494b3b5c7ce9eb17ee3.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:49298 - \"\u001b[1mPOST /v1/chat/completions?response_format%5Btype%5D=json_object&guide_json%5Btype%5D=object&guide_json%5Bproperties%5D%5Bfirst_key%5D%5Btype%5D=array&guide_json%5Bproperties%5D%5Bfirst_key%5D%5Bitems%5D%5Btype%5D=string&guide_json%5Bproperties%5D%5Bfirst_key%5D%5BminItems%5D=1&guide_json%5Bproperties%5D%5Banother_key%5D%5Btype%5D=array&guide_json%5Bproperties%5D%5Banother_key%5D%5Bitems%5D%5Btype%5D=string&guide_json%5Bproperties%5D%5Banother_key%5D%5BminItems%5D=1&guide_json%5Bproperties%5D%5Brequired%5D=first_key%2Canother_key HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-22 15:27:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:27:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:27:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:27:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:28:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:28:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:28:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:28:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:28:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:28:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:29:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:29:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:29:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:29:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:29:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:29:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:30:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:30:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:30:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:30:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:30:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:30:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:31:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:31:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:31:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:31:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:31:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:31:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:32:00 async_llm_engine.py:524] Received request cmpl-db175601a0dc470cb138a8d888a42dc2: prompt: '<|im_start|>user\\n你是一位资深医学专家，目前需要从报告\\'检查所见\\'中的关于肺结节的核心字段，主要包括\\'原文句子\\',\\'结节部位\\',\\'结节大小\\',\\'结节类型\\',\\'结节边界\\', \\'淋巴结情况\\', 需要从\\'检查诊断\\'中的信息用于提取“LUNG-RADs”字段，然后输出一个json格便于后续的处理, 返回形式类似 list of dict 格式:\\n\\n#要求：\\n1、请输出一个合法的 JSON 字符串, 参考 sample output 的输出格式;\\n2、“检查所见”字段信息需要抽取对应原文，找到肺结节相关信息;\\n3、“检查所见”字段信息不需要抽取原文，需要协助找到\"LUNG-RADs\"的值，只有“结节类型”为肺结节类型时，才能有值，否则为空\"-\";\\n\\n\\n#example:\\n#sample input：\\n检查所见: 左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断: 对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#sample output:\\n[\\n    {\\n        \\'原文句子\\': \\'左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚\\',\\n        \\'结节部位\\': \\'左肺上、下叶\\',\\n        \\'结节大小\\': \\'-\\',\\n        \\'结节类型\\': \\'团片状、斑点、结节高密度影,钙化\\',\\n        \\'结节边界\\': \\'边界不清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n    {\\n        \\'原文句子\\': \\'另两肺见多发小结节影，直径约3-4mm，界清\\',\\n        \\'结节部位\\': \\'两肺\\',\\n        \\'结节大小\\': \\'直径约3-4mm\\',\\n        \\'结节类型\\': \\'小结节影\\',\\n        \\'结节边界\\': \\'界清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n]\\n\\n#input:\\n检查所见:左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断:对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#output:\\n\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=3093, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 872, 198, 56568, 109182, 107505, 104316, 101057, 3837, 100004, 85106, 45181, 100370, 6, 101071, 31838, 88970, 6, 101047, 101888, 100887, 36885, 55502, 104867, 44931, 3837, 110800, 6, 103283, 109949, 1844, 36885, 55502, 105130, 1844, 36885, 55502, 92032, 1844, 36885, 55502, 31905, 1844, 36885, 55502, 108758, 516, 364, 114814, 36885, 99559, 516, 18137, 250, 222, 30534, 45181, 6, 101071, 105262, 6, 101047, 27369, 100751, 107439, 2073, 43, 60002, 10911, 1808, 82, 854, 44931, 3837, 101889, 66017, 46944, 2236, 33983, 108085, 105463, 9370, 54542, 11, 55616, 100414, 101412, 1140, 315, 6451, 51461, 120, 28330, 1447, 2, 101882, 28311, 16, 5373, 14880, 66017, 46944, 101431, 9370, 4718, 73312, 38304, 51575, 11, 26853, 224, 77598, 6077, 2550, 43589, 66017, 68805, 280, 17, 91956, 101071, 31838, 88970, 854, 44931, 27369, 85106, 118797, 103124, 103283, 3837, 101958, 100887, 36885, 55502, 110984, 280, 18, 91956, 101071, 31838, 88970, 854, 44931, 27369, 104689, 118797, 103283, 3837, 85106, 105096, 101958, 73630, 60002, 10911, 1808, 82, 1, 9370, 25511, 3837, 101043, 2073, 36885, 55502, 31905, 854, 17714, 100887, 36885, 55502, 31905, 13343, 3837, 101901, 18830, 25511, 3837, 104420, 50647, 34294, 29122, 2, 8687, 510, 2, 13611, 1946, 28311, 101071, 31838, 88970, 25, 83002, 99, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 69162, 56006, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 13611, 2550, 510, 9640, 262, 341, 286, 364, 103283, 109949, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 751, 286, 364, 36885, 55502, 92032, 1210, 7788, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 11, 106033, 32108, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 108758, 106122, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 262, 341, 286, 364, 103283, 109949, 1210, 364, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77540, 100887, 751, 286, 364, 36885, 55502, 92032, 1210, 364, 109808, 94237, 18, 12, 19, 3821, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 30709, 36885, 55502, 57222, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 97120, 79766, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 2533, 2, 1355, 510, 101071, 31838, 88970, 25, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 104877, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 3006, 1447, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-22 15:32:02 metrics.py:229] Avg prompt throughput: 200.1 tokens/s, Avg generation throughput: 12.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:32:07 async_llm_engine.py:120] Finished request cmpl-db175601a0dc470cb138a8d888a42dc2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:37236 - \"\u001b[1mPOST /v1/chat/completions?response_format%5Btype%5D=jsobject&guided_json%5Btype%5D=object&guided_json%5Bproperties%5D%5Bfirst_key%5D%5Btype%5D=array&guided_json%5Bproperties%5D%5Bfirst_key%5D%5Bitems%5D%5Btype%5D=string&guided_json%5Bproperties%5D%5Bfirst_key%5D%5BminItems%5D=1&guided_json%5Bproperties%5D%5Banother_key%5D%5Btype%5D=array&guided_json%5Bproperties%5D%5Banother_key%5D%5Bitems%5D%5Btype%5D=string&guided_json%5Bproperties%5D%5Banother_key%5D%5BminItems%5D=1&guided_json%5Bproperties%5D%5Brequired%5D=first_key%2Canother_key HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-22 15:32:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:32:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:32:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:32:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:32:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:33:03 async_llm_engine.py:524] Received request cmpl-823c05a656f24b37a6063b11d47cb86d: prompt: '<|im_start|>user\\n你是一位资深医学专家，目前需要从报告\\'检查所见\\'中的关于肺结节的核心字段，主要包括\\'原文句子\\',\\'结节部位\\',\\'结节大小\\',\\'结节类型\\',\\'结节边界\\', \\'淋巴结情况\\', 需要从\\'检查诊断\\'中的信息用于提取“LUNG-RADs”字段，然后输出一个json格便于后续的处理, 返回形式类似 list of dict 格式:\\n\\n#要求：\\n1、请输出一个合法的 JSON 字符串, 参考 sample output 的输出格式;\\n2、“检查所见”字段信息需要抽取对应原文，找到肺结节相关信息;\\n3、“检查所见”字段信息不需要抽取原文，需要协助找到\"LUNG-RADs\"的值，只有“结节类型”为肺结节类型时，才能有值，否则为空\"-\";\\n\\n\\n#example:\\n#sample input：\\n检查所见: 左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断: 对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#sample output:\\n[\\n    {\\n        \\'原文句子\\': \\'左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚\\',\\n        \\'结节部位\\': \\'左肺上、下叶\\',\\n        \\'结节大小\\': \\'-\\',\\n        \\'结节类型\\': \\'团片状、斑点、结节高密度影,钙化\\',\\n        \\'结节边界\\': \\'边界不清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n    {\\n        \\'原文句子\\': \\'另两肺见多发小结节影，直径约3-4mm，界清\\',\\n        \\'结节部位\\': \\'两肺\\',\\n        \\'结节大小\\': \\'直径约3-4mm\\',\\n        \\'结节类型\\': \\'小结节影\\',\\n        \\'结节边界\\': \\'界清\\',\\n        \\'淋巴结情况\\': \\'-\\',\\n        \\'LUNG-RADs\\': \\'2类\\',\\n    },\\n]\\n\\n#input:\\n检查所见:左肺上、下叶见多发团片状、斑点、结节高密度影，边界不清，左肺上叶为著伴钙化，周围见多发类结节影，邻近胸膜粘连增厚；左肺上叶体积缩小；左侧胸腔内可见积气、积液及引流管影；另两肺见多发小结节影，直径约3-4mm，界清；余两肺纹理略增多，未见明显异常密度占位影；两肺门结构尚清，纵隔居中，其内未见明显肿大淋巴结；右侧胸腔、心包腔内未见明显积液影；心影无殊.\\n\\n检查诊断:对比2023-03-29片： 1.左肺多发多形性病变，左肺上叶为著，左侧胸膜增厚，VP-RADS 2类，结核首先考虑，请结合临床实验室检查及治疗后复查。 2.左侧液气胸胸腔闭式引流术后改变，积气较前略吸收。 3.两肺多发小结节，LUNG-RADs:2类，大致相仿，建议年度复查。 附见：肝内致密影。\\n\\n#output:\\n\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.8, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=3093, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 872, 198, 56568, 109182, 107505, 104316, 101057, 3837, 100004, 85106, 45181, 100370, 6, 101071, 31838, 88970, 6, 101047, 101888, 100887, 36885, 55502, 104867, 44931, 3837, 110800, 6, 103283, 109949, 1844, 36885, 55502, 105130, 1844, 36885, 55502, 92032, 1844, 36885, 55502, 31905, 1844, 36885, 55502, 108758, 516, 364, 114814, 36885, 99559, 516, 18137, 250, 222, 30534, 45181, 6, 101071, 105262, 6, 101047, 27369, 100751, 107439, 2073, 43, 60002, 10911, 1808, 82, 854, 44931, 3837, 101889, 66017, 46944, 2236, 33983, 108085, 105463, 9370, 54542, 11, 55616, 100414, 101412, 1140, 315, 6451, 51461, 120, 28330, 1447, 2, 101882, 28311, 16, 5373, 14880, 66017, 46944, 101431, 9370, 4718, 73312, 38304, 51575, 11, 26853, 224, 77598, 6077, 2550, 43589, 66017, 68805, 280, 17, 91956, 101071, 31838, 88970, 854, 44931, 27369, 85106, 118797, 103124, 103283, 3837, 101958, 100887, 36885, 55502, 110984, 280, 18, 91956, 101071, 31838, 88970, 854, 44931, 27369, 104689, 118797, 103283, 3837, 85106, 105096, 101958, 73630, 60002, 10911, 1808, 82, 1, 9370, 25511, 3837, 101043, 2073, 36885, 55502, 31905, 854, 17714, 100887, 36885, 55502, 31905, 13343, 3837, 101901, 18830, 25511, 3837, 104420, 50647, 34294, 29122, 2, 8687, 510, 2, 13611, 1946, 28311, 101071, 31838, 88970, 25, 83002, 99, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 69162, 56006, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 13611, 2550, 510, 9640, 262, 341, 286, 364, 103283, 109949, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77559, 100887, 17447, 5373, 16872, 100027, 751, 286, 364, 36885, 55502, 92032, 1210, 7788, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 11, 106033, 32108, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 108758, 106122, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 262, 341, 286, 364, 103283, 109949, 1210, 364, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 751, 286, 364, 36885, 55502, 105130, 1210, 364, 77540, 100887, 751, 286, 364, 36885, 55502, 92032, 1210, 364, 109808, 94237, 18, 12, 19, 3821, 751, 286, 364, 36885, 55502, 31905, 1210, 364, 30709, 36885, 55502, 57222, 751, 286, 364, 36885, 55502, 108758, 1210, 364, 97120, 79766, 751, 286, 364, 114814, 36885, 99559, 1210, 7788, 751, 286, 364, 43, 60002, 10911, 1808, 82, 1210, 364, 17, 21515, 751, 262, 1153, 2533, 2, 1355, 510, 101071, 31838, 88970, 25, 77559, 100887, 17447, 5373, 16872, 100027, 88970, 42140, 28291, 99305, 34718, 99762, 5373, 100876, 27442, 5373, 36885, 55502, 44636, 106651, 57222, 3837, 108758, 106122, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 99595, 106033, 32108, 3837, 102385, 88970, 42140, 28291, 21515, 36885, 55502, 57222, 3837, 100603, 59258, 100277, 101232, 104957, 54926, 49185, 99696, 24968, 77559, 100887, 17447, 100027, 110238, 108287, 24968, 111687, 100277, 102504, 31843, 101479, 99263, 99180, 5373, 99263, 100058, 81217, 117273, 35551, 57222, 24968, 99628, 77540, 100887, 88970, 42140, 28291, 30709, 36885, 55502, 57222, 3837, 109808, 94237, 18, 12, 19, 3821, 3837, 97120, 79766, 24968, 88683, 77540, 100887, 118526, 99475, 107284, 3837, 38342, 88970, 100687, 70633, 106651, 99571, 24156, 57222, 24968, 77540, 100887, 64689, 100166, 99937, 79766, 3837, 100306, 99859, 99435, 15946, 3837, 41146, 31843, 38342, 88970, 100687, 101561, 26288, 114814, 36885, 24968, 113658, 100277, 102504, 5373, 63109, 67279, 102504, 31843, 38342, 88970, 100687, 99263, 100058, 57222, 24968, 63109, 57222, 42192, 100747, 382, 101071, 105262, 25, 104877, 17, 15, 17, 18, 12, 15, 18, 12, 17, 24, 34718, 5122, 220, 16, 13, 77559, 100887, 42140, 28291, 42140, 82699, 33071, 112140, 3837, 77559, 100887, 17447, 100027, 17714, 99610, 3837, 111687, 100277, 101232, 49185, 99696, 3837, 13378, 10911, 49541, 220, 17, 21515, 3837, 36885, 71137, 101140, 101118, 37945, 100374, 104595, 104800, 101071, 81217, 101899, 33447, 118287, 1773, 220, 17, 13, 111687, 100058, 99180, 100277, 100277, 102504, 58792, 28330, 117273, 111080, 101933, 3837, 99263, 99180, 99260, 24562, 99475, 104460, 1773, 220, 18, 13, 77540, 100887, 42140, 28291, 30709, 36885, 55502, 3837, 43, 60002, 10911, 1808, 82, 25, 17, 21515, 3837, 108172, 48921, 100863, 3837, 101898, 103986, 118287, 1773, 220, 100093, 88970, 5122, 101364, 31843, 99299, 27641, 57222, 3407, 2, 3006, 1447, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-22 15:33:04 metrics.py:229] Avg prompt throughput: 149.8 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:33:09 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 37.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.3%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:33:10 async_llm_engine.py:120] Finished request cmpl-823c05a656f24b37a6063b11d47cb86d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47924 - \"\u001b[1mPOST /v1/chat/completions?guided_on%5Btype%5D=object&guided_on%5Bproperties%5D%5Bfirst_key%5D%5Btype%5D=array&guided_on%5Bproperties%5D%5Bfirst_key%5D%5Bitems%5D%5Btype%5D=string&guided_on%5Bproperties%5D%5Bfirst_key%5D%5BminItems%5D=1&guided_on%5Bproperties%5D%5Banother_key%5D%5Btype%5D=array&guided_on%5Bproperties%5D%5Banother_key%5D%5Bitems%5D%5Btype%5D=string&guided_on%5Bproperties%5D%5Banother_key%5D%5BminItems%5D=1&guided_on%5Bproperties%5D%5Brequired%5D=first_key%2Canother_key HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-22 15:33:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:33:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:33:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:33:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:33:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:34:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:34:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:34:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:34:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:34:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:34:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:35:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:35:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:42168 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[91m500 Internal Server Error\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 419, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\n",
      "    raise e\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 90, in create_chat_completion\n",
      "    generator = await openai_serving_chat.create_chat_completion(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 75, in create_chat_completion\n",
      "    await get_guided_decoding_logits_processor(\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/__init__.py\", line 17, in get_guided_decoding_logits_processor\n",
      "    return await get_outlines_guided_decoding_logits_processor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 75, in get_outlines_guided_decoding_logits_processor\n",
      "    result = await loop.run_in_executor(global_thread_pool,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 124, in _get_cached_logits_processor\n",
      "    return RegexLogitsProcessor(guide, tokenizer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 77, in __init__\n",
      "    fsm = RegexFSM(regex_string, tokenizer)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 121, in __init__\n",
      "    self.states_to_token_maps, self.empty_token_ids = create_states_mapping(\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/caching.py\", line 74, in wrapper\n",
      "    result = cached_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 104, in create_states_mapping\n",
      "    states_to_token_maps, empty_token_ids = create_fsm_index_tokenizer(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 571, in create_fsm_index_tokenizer\n",
      "    vocabulary, empty_token_ids = reduced_vocabulary(tokenizer)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 545, in reduced_vocabulary\n",
      "    token_str = tokenizer.convert_token_to_string(token)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 166, in convert_token_to_string\n",
      "    if token.startswith(SPIECE_UNDERLINE) or token == \"<0x20>\":\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: startswith first arg must be bytes or a tuple of bytes, not str\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:42182 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[91m500 Internal Server Error\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 419, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\n",
      "    raise e\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 90, in create_chat_completion\n",
      "    generator = await openai_serving_chat.create_chat_completion(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 75, in create_chat_completion\n",
      "    await get_guided_decoding_logits_processor(\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/__init__.py\", line 17, in get_guided_decoding_logits_processor\n",
      "    return await get_outlines_guided_decoding_logits_processor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 75, in get_outlines_guided_decoding_logits_processor\n",
      "    result = await loop.run_in_executor(global_thread_pool,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 124, in _get_cached_logits_processor\n",
      "    return RegexLogitsProcessor(guide, tokenizer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 77, in __init__\n",
      "    fsm = RegexFSM(regex_string, tokenizer)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 121, in __init__\n",
      "    self.states_to_token_maps, self.empty_token_ids = create_states_mapping(\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/caching.py\", line 74, in wrapper\n",
      "    result = cached_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 104, in create_states_mapping\n",
      "    states_to_token_maps, empty_token_ids = create_fsm_index_tokenizer(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 571, in create_fsm_index_tokenizer\n",
      "    vocabulary, empty_token_ids = reduced_vocabulary(tokenizer)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 545, in reduced_vocabulary\n",
      "    token_str = tokenizer.convert_token_to_string(token)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 166, in convert_token_to_string\n",
      "    if token.startswith(SPIECE_UNDERLINE) or token == \"<0x20>\":\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: startswith first arg must be bytes or a tuple of bytes, not str\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:42184 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[91m500 Internal Server Error\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 419, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\n",
      "    raise e\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 90, in create_chat_completion\n",
      "    generator = await openai_serving_chat.create_chat_completion(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 75, in create_chat_completion\n",
      "    await get_guided_decoding_logits_processor(\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/__init__.py\", line 17, in get_guided_decoding_logits_processor\n",
      "    return await get_outlines_guided_decoding_logits_processor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 75, in get_outlines_guided_decoding_logits_processor\n",
      "    result = await loop.run_in_executor(global_thread_pool,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 124, in _get_cached_logits_processor\n",
      "    return RegexLogitsProcessor(guide, tokenizer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 77, in __init__\n",
      "    fsm = RegexFSM(regex_string, tokenizer)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 121, in __init__\n",
      "    self.states_to_token_maps, self.empty_token_ids = create_states_mapping(\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/caching.py\", line 74, in wrapper\n",
      "    result = cached_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 104, in create_states_mapping\n",
      "    states_to_token_maps, empty_token_ids = create_fsm_index_tokenizer(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 571, in create_fsm_index_tokenizer\n",
      "    vocabulary, empty_token_ids = reduced_vocabulary(tokenizer)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 545, in reduced_vocabulary\n",
      "    token_str = tokenizer.convert_token_to_string(token)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 166, in convert_token_to_string\n",
      "    if token.startswith(SPIECE_UNDERLINE) or token == \"<0x20>\":\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: startswith first arg must be bytes or a tuple of bytes, not str\n",
      "INFO 05-22 15:35:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:35:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:35:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:35:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:36:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:48818 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[91m500 Internal Server Error\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 419, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\n",
      "    raise e\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 90, in create_chat_completion\n",
      "    generator = await openai_serving_chat.create_chat_completion(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 75, in create_chat_completion\n",
      "    await get_guided_decoding_logits_processor(\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/__init__.py\", line 17, in get_guided_decoding_logits_processor\n",
      "    return await get_outlines_guided_decoding_logits_processor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 75, in get_outlines_guided_decoding_logits_processor\n",
      "    result = await loop.run_in_executor(global_thread_pool,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 124, in _get_cached_logits_processor\n",
      "    return RegexLogitsProcessor(guide, tokenizer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 77, in __init__\n",
      "    fsm = RegexFSM(regex_string, tokenizer)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 121, in __init__\n",
      "    self.states_to_token_maps, self.empty_token_ids = create_states_mapping(\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/caching.py\", line 74, in wrapper\n",
      "    result = cached_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 104, in create_states_mapping\n",
      "    states_to_token_maps, empty_token_ids = create_fsm_index_tokenizer(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 571, in create_fsm_index_tokenizer\n",
      "    vocabulary, empty_token_ids = reduced_vocabulary(tokenizer)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 545, in reduced_vocabulary\n",
      "    token_str = tokenizer.convert_token_to_string(token)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 166, in convert_token_to_string\n",
      "    if token.startswith(SPIECE_UNDERLINE) or token == \"<0x20>\":\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: startswith first arg must be bytes or a tuple of bytes, not str\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:48820 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[91m500 Internal Server Error\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 419, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\n",
      "    raise e\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 90, in create_chat_completion\n",
      "    generator = await openai_serving_chat.create_chat_completion(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 75, in create_chat_completion\n",
      "    await get_guided_decoding_logits_processor(\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/__init__.py\", line 17, in get_guided_decoding_logits_processor\n",
      "    return await get_outlines_guided_decoding_logits_processor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 75, in get_outlines_guided_decoding_logits_processor\n",
      "    result = await loop.run_in_executor(global_thread_pool,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 124, in _get_cached_logits_processor\n",
      "    return RegexLogitsProcessor(guide, tokenizer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 77, in __init__\n",
      "    fsm = RegexFSM(regex_string, tokenizer)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 121, in __init__\n",
      "    self.states_to_token_maps, self.empty_token_ids = create_states_mapping(\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/caching.py\", line 74, in wrapper\n",
      "    result = cached_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 104, in create_states_mapping\n",
      "    states_to_token_maps, empty_token_ids = create_fsm_index_tokenizer(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 571, in create_fsm_index_tokenizer\n",
      "    vocabulary, empty_token_ids = reduced_vocabulary(tokenizer)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 545, in reduced_vocabulary\n",
      "    token_str = tokenizer.convert_token_to_string(token)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 166, in convert_token_to_string\n",
      "    if token.startswith(SPIECE_UNDERLINE) or token == \"<0x20>\":\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: startswith first arg must be bytes or a tuple of bytes, not str\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:34330 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[91m500 Internal Server Error\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 419, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 83, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\n",
      "    response = await func(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\n",
      "    raise e\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 90, in create_chat_completion\n",
      "    generator = await openai_serving_chat.create_chat_completion(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 75, in create_chat_completion\n",
      "    await get_guided_decoding_logits_processor(\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/__init__.py\", line 17, in get_guided_decoding_logits_processor\n",
      "    return await get_outlines_guided_decoding_logits_processor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 75, in get_outlines_guided_decoding_logits_processor\n",
      "    result = await loop.run_in_executor(global_thread_pool,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_decoding.py\", line 124, in _get_cached_logits_processor\n",
      "    return RegexLogitsProcessor(guide, tokenizer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 77, in __init__\n",
      "    fsm = RegexFSM(regex_string, tokenizer)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 121, in __init__\n",
      "    self.states_to_token_maps, self.empty_token_ids = create_states_mapping(\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/caching.py\", line 74, in wrapper\n",
      "    result = cached_function(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/fsm.py\", line 104, in create_states_mapping\n",
      "    states_to_token_maps, empty_token_ids = create_fsm_index_tokenizer(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 571, in create_fsm_index_tokenizer\n",
      "    vocabulary, empty_token_ids = reduced_vocabulary(tokenizer)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/outlines/fsm/regex.py\", line 545, in reduced_vocabulary\n",
      "    token_str = tokenizer.convert_token_to_string(token)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/guided_decoding/outlines_logits_processors.py\", line 166, in convert_token_to_string\n",
      "    if token.startswith(SPIECE_UNDERLINE) or token == \"<0x20>\":\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: startswith first arg must be bytes or a tuple of bytes, not str\n",
      "INFO 05-22 15:36:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:36:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:36:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:36:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:36:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:37:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:37:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:37:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:37:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:37:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:37:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:38:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:38:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:38:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:38:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:38:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:38:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:39:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:39:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:39:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:39:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:39:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:39:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:40:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:40:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:40:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:40:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:40:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:40:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:41:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:41:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:41:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:41:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:41:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:41:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:42:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:42:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:42:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:42:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:42:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:42:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:43:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:43:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:43:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:43:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:43:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:43:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:44:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:44:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:44:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:44:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:44:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:44:57 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:45:07 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:45:17 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:45:27 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-22 15:45:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m2719970\u001b[0m]\n",
      "Exception in callback functools.partial(<function _raise_exception_on_finish at 0x7ff382b27060>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7ff358624410>>)\n",
      "handle: <Handle functools.partial(<function _raise_exception_on_finish at 0x7ff382b27060>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7ff358624410>>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\n",
      "    task.result()\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 490, in run_engine_loop\n",
      "    await self._request_tracker.wait_for_new_requests()\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 189, in wait_for_new_requests\n",
      "    await self.new_requests_event.wait()\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/asyncio/locks.py\", line 213, in wait\n",
      "    await fut\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "# 量化版本\n",
    "!python -m vllm.entrypoints.openai.api_server --served-model-name Qwen1.5-32B-Chat-GPTQ \\\n",
    "      --model Qwen/Qwen-72B-Chat-Int4/  --port 1122 --quantization gptq \\\n",
    "      --gpu-memory-utilization 0.8 --tensor-parallel-size 2 --trust-remote-code   --max-model-len 4096 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db3eeae7-337d-44d4-989b-2bb7f1df2456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vllm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vllm\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vllm' is not defined"
     ]
    }
   ],
   "source": [
    "vllm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7b6781-fd4a-4eba-8763-b129346d7961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: api_server.py [-h] [--host HOST] [--port PORT]\n",
      "                     [--uvicorn-log-level {debug,info,warning,error,critical,trace}]\n",
      "                     [--allow-credentials] [--allowed-origins ALLOWED_ORIGINS]\n",
      "                     [--allowed-methods ALLOWED_METHODS]\n",
      "                     [--allowed-headers ALLOWED_HEADERS] [--api-key API_KEY]\n",
      "                     [--served-model-name SERVED_MODEL_NAME]\n",
      "                     [--lora-modules LORA_MODULES [LORA_MODULES ...]]\n",
      "                     [--chat-template CHAT_TEMPLATE]\n",
      "                     [--response-role RESPONSE_ROLE]\n",
      "                     [--ssl-keyfile SSL_KEYFILE] [--ssl-certfile SSL_CERTFILE]\n",
      "                     [--ssl-ca-certs SSL_CA_CERTS]\n",
      "                     [--ssl-cert-reqs SSL_CERT_REQS] [--root-path ROOT_PATH]\n",
      "                     [--middleware MIDDLEWARE] [--model MODEL]\n",
      "                     [--tokenizer TOKENIZER] [--revision REVISION]\n",
      "                     [--code-revision CODE_REVISION]\n",
      "                     [--tokenizer-revision TOKENIZER_REVISION]\n",
      "                     [--tokenizer-mode {auto,slow}] [--trust-remote-code]\n",
      "                     [--download-dir DOWNLOAD_DIR]\n",
      "                     [--load-format {auto,pt,safetensors,npcache,dummy}]\n",
      "                     [--dtype {auto,half,float16,bfloat16,float,float32}]\n",
      "                     [--kv-cache-dtype {auto,fp8_e5m2}]\n",
      "                     [--max-model-len MAX_MODEL_LEN] [--worker-use-ray]\n",
      "                     [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]\n",
      "                     [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n",
      "                     [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]\n",
      "                     [--ray-workers-use-nsight] [--block-size {8,16,32,128}]\n",
      "                     [--enable-prefix-caching] [--use-v2-block-manager]\n",
      "                     [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS] [--seed SEED]\n",
      "                     [--swap-space SWAP_SPACE]\n",
      "                     [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]\n",
      "                     [--forced-num-gpu-blocks FORCED_NUM_GPU_BLOCKS]\n",
      "                     [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]\n",
      "                     [--max-num-seqs MAX_NUM_SEQS]\n",
      "                     [--max-logprobs MAX_LOGPROBS] [--disable-log-stats]\n",
      "                     [--quantization {awq,gptq,squeezellm,None}]\n",
      "                     [--enforce-eager]\n",
      "                     [--max-context-len-to-capture MAX_CONTEXT_LEN_TO_CAPTURE]\n",
      "                     [--disable-custom-all-reduce]\n",
      "                     [--tokenizer-pool-size TOKENIZER_POOL_SIZE]\n",
      "                     [--tokenizer-pool-type TOKENIZER_POOL_TYPE]\n",
      "                     [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]\n",
      "                     [--enable-lora] [--max-loras MAX_LORAS]\n",
      "                     [--max-lora-rank MAX_LORA_RANK]\n",
      "                     [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]\n",
      "                     [--lora-dtype {auto,float16,bfloat16,float32}]\n",
      "                     [--max-cpu-loras MAX_CPU_LORAS]\n",
      "                     [--device {auto,cuda,neuron,cpu}]\n",
      "                     [--image-input-type {pixel_values,image_features}]\n",
      "                     [--image-token-id IMAGE_TOKEN_ID]\n",
      "                     [--image-input-shape IMAGE_INPUT_SHAPE]\n",
      "                     [--image-feature-size IMAGE_FEATURE_SIZE]\n",
      "                     [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]\n",
      "                     [--enable-chunked-prefill ENABLE_CHUNKED_PREFILL]\n",
      "                     [--engine-use-ray] [--disable-log-requests]\n",
      "                     [--max-log-len MAX_LOG_LEN]\n",
      "api_server.py: error: unrecognized arguments: --served-model-nme Qwen1.5-14B-Chat\n"
     ]
    }
   ],
   "source": [
    "# 非量化版本: 7B 模型\n",
    "!python -m vllm.entrypoints.openai.api_server --served-model-name Qwen1.5-7B-Chat \\\n",
    "      --model Qwen/Qwen1___5-14B-Chat  --port 1122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46159302-0946-4e34-99af-2eda83da6ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[1;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection.py:122\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 122\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    123\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_backends/sync.py:205\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    203\u001b[0m }\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    206\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    207\u001b[0m         address,\n\u001b[1;32m    208\u001b[0m         timeout,\n\u001b[1;32m    209\u001b[0m         source_address\u001b[38;5;241m=\u001b[39msource_address,\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:898\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 898\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    899\u001b[0m         request,\n\u001b[1;32m    900\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m    901\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    902\u001b[0m     )\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:232\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m openai_api_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:1122/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      7\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_api_key,\n\u001b[1;32m      8\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mopenai_api_base,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen1.5-7B-Chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     14\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你是一个人工智能助手.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     15\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m告诉我有关北京的特产\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     16\u001b[0m     ]\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_response)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    661\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    662\u001b[0m             {\n\u001b[1;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    683\u001b[0m             },\n\u001b[1;32m    684\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    685\u001b[0m         ),\n\u001b[1;32m    686\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    687\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    688\u001b[0m         ),\n\u001b[1;32m    689\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    690\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    692\u001b[0m     )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:1180\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1177\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1178\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1179\u001b[0m     )\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:869\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    862\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    870\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    871\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    872\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    873\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    874\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    875\u001b[0m     )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    923\u001b[0m         options,\n\u001b[1;32m    924\u001b[0m         cast_to,\n\u001b[1;32m    925\u001b[0m         retries,\n\u001b[1;32m    926\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    927\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    928\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    994\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    995\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    996\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m    997\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    998\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    999\u001b[0m )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    923\u001b[0m         options,\n\u001b[1;32m    924\u001b[0m         cast_to,\n\u001b[1;32m    925\u001b[0m         retries,\n\u001b[1;32m    926\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    927\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    928\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    994\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    995\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    996\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m    997\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    998\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    999\u001b[0m )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:932\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    923\u001b[0m             options,\n\u001b[1;32m    924\u001b[0m             cast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m             response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    929\u001b[0m         )\n\u001b[1;32m    931\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 932\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    934\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl, response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mreason_phrase\n\u001b[1;32m    936\u001b[0m )\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:1122/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"Qwen1.5-7B-Chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个人工智能助手.\"},\n",
    "        {\"role\": \"user\", \"content\": \"告诉我有关北京的特产\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82b5cb-c305-4537-867b-045b33110486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
