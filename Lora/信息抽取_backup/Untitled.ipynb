{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "767cdd92-d42a-4335-95b2-d00bbd76fc27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: ChatCompletion(id='cmpl-3280735e31e54a6a856eae49efec5235', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='北京的特产有“北京烤鸭”、“豆汁焦圈”、“羊蝎子”、“艾窝窝”、“冰糖葫芦”等，它们都是北京独有的特色美食。此外，还有“景泰蓝”、“北京绢人”、“雕漆”等工艺品，以及“瓷器”、“茶叶”等传统文化产品。', role='assistant', function_call=None, tool_calls=None), stop_reason=None)], created=1715767771, model='quant_lora', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=71, prompt_tokens=23, total_tokens=94))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:1122/v1\"\n",
    "import time\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"quant_lora\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个人工智能助手.\"},\n",
    "        {\"role\": \"user\", \"content\": \"告诉我有关北京的特产\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb83b68-003f-491f-ad04-e78870894d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"cuda_visible_devices\"] ='0, 5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d217332-19c8-46c1-8b03-4786a95fbeeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-15 17:48:44 config.py:211] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-15 17:48:44 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='gptq_qwen/', tokenizer='gptq_qwen/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "WARNING 05-15 17:48:44 tokenizer.py:104] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 05-15 17:48:48 selector.py:16] Using FlashAttention backend.\n",
      "INFO 05-15 17:48:55 model_runner.py:104] Loading model weights took 5.5044 GB\n",
      "INFO 05-15 17:48:57 gpu_executor.py:94] # GPU blocks: 3594, # CPU blocks: 512\n",
      "INFO 05-15 17:49:04 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-15 17:49:04 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-15 17:49:20 model_runner.py:867] Graph capturing finished in 16 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:26<00:00,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.685515642166138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:30<00:00,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.81380271911621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "\n",
    "# Sample prompts.\n",
    "prompts = [\n",
    "    \"告诉我有关北京的特产\",\n",
    "] * 5\n",
    "\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=2000)\n",
    "\n",
    "# Create an LLM.\n",
    "llm = LLM(model='gptq_qwen/',quantization='gptq',\n",
    "         trust_remote_code=True)\n",
    "\n",
    "prompt_template='''\n",
    "给定一句话：“%s”，请你按步骤要求工作。\n",
    "\n",
    "步骤1：识别这句话中的城市和日期共2个信息\n",
    "步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\n",
    "\n",
    "请问，这个JSON字符串是：\n",
    "'''\n",
    "\n",
    "Q_list=['2020年4月16号三亚下雨么?','青岛3-15号天气预报','5月6号下雪么, 城市是威海','青岛2023年12月30号有雾霾么?','我打算6月1号去北京旅游，请问天气怎么样？','你们打算1月3号坐哪一趟航班去上海？','小明和小红是8月8号在上海结婚么?',\n",
    "        '一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起']\n",
    "\n",
    "prompts = [prompt_template%(Q,) for Q in Q_list]\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8097d2-15e3-47e3-a7a1-7018575b3fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f406a088-baaa-4bd4-a06b-d6b47d276ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                                                                                                           | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m告诉我有关上海的特产\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m ] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mgenerate(prompts, sampling_params)\n\u001b[1;32m      8\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtoken_ids) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]) \n\u001b[1;32m      9\u001b[0m spend_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/llm.py:190\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, multi_modal_data)\u001b[0m\n\u001b[1;32m    177\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    178\u001b[0m         i]\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_request(\n\u001b[1;32m    180\u001b[0m         prompt,\n\u001b[1;32m    181\u001b[0m         sampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m multi_modal_data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     )\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_engine(use_tqdm)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/llm.py:218\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    216\u001b[0m outputs: List[RequestOutput] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 218\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/llm_engine.py:676\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[0;32m--> 676\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor\u001b[38;5;241m.\u001b[39mexecute_model(\n\u001b[1;32m    677\u001b[0m         seq_group_metadata_list, scheduler_outputs\u001b[38;5;241m.\u001b[39mblocks_to_swap_in,\n\u001b[1;32m    678\u001b[0m         scheduler_outputs\u001b[38;5;241m.\u001b[39mblocks_to_swap_out,\n\u001b[1;32m    679\u001b[0m         scheduler_outputs\u001b[38;5;241m.\u001b[39mblocks_to_copy)\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    681\u001b[0m     output \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:114\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    110\u001b[0m                   seq_group_metadata_list: List[SequenceGroupMetadata],\n\u001b[1;32m    111\u001b[0m                   blocks_to_swap_in: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    112\u001b[0m                   blocks_to_swap_out: Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    113\u001b[0m                   blocks_to_copy: Dict[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SamplerOutput:\n\u001b[0;32m--> 114\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39mexecute_model(\n\u001b[1;32m    115\u001b[0m         seq_group_metadata_list\u001b[38;5;241m=\u001b[39mseq_group_metadata_list,\n\u001b[1;32m    116\u001b[0m         blocks_to_swap_in\u001b[38;5;241m=\u001b[39mblocks_to_swap_in,\n\u001b[1;32m    117\u001b[0m         blocks_to_swap_out\u001b[38;5;241m=\u001b[39mblocks_to_swap_out,\n\u001b[1;32m    118\u001b[0m         blocks_to_copy\u001b[38;5;241m=\u001b[39mblocks_to_copy,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/worker/worker.py:221\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 221\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_runner\u001b[38;5;241m.\u001b[39mexecute_model(seq_group_metadata_list,\n\u001b[1;32m    222\u001b[0m                                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/worker/model_runner.py:673\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    674\u001b[0m     logits\u001b[38;5;241m=\u001b[39mlogits,\n\u001b[1;32m    675\u001b[0m     sampling_metadata\u001b[38;5;241m=\u001b[39msampling_metadata,\n\u001b[1;32m    676\u001b[0m )\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py:254\u001b[0m, in \u001b[0;36mQWenLMHeadModel.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    251\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    252\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 254\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler(logits, sampling_metadata)\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:76\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m     73\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m sample_results \u001b[38;5;241m=\u001b[39m _sample(probs, logprobs, sampling_metadata,\n\u001b[1;32m     77\u001b[0m                          sampling_tensors)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Get the logprobs query results.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m prompt_logprobs, sample_logprobs \u001b[38;5;241m=\u001b[39m _get_logprobs(\n\u001b[1;32m     80\u001b[0m     logprobs, sampling_metadata, sample_results)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:502\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sample\u001b[39m(\n\u001b[1;32m    497\u001b[0m     probs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    498\u001b[0m     logprobs: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    499\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    500\u001b[0m     sampling_tensors: SamplingTensors,\n\u001b[1;32m    501\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sample_with_torch(probs, logprobs, sampling_metadata)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:401\u001b[0m, in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata)\u001b[0m\n\u001b[1;32m    399\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _greedy_sample(seq_groups, greedy_samples)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType\u001b[38;5;241m.\u001b[39mRANDOM, SamplingType\u001b[38;5;241m.\u001b[39mRANDOM_SEED):\n\u001b[0;32m--> 401\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _random_sample(seq_groups, is_prompts,\n\u001b[1;32m    402\u001b[0m                                     multinomial_samples[sampling_type])\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mBEAM:\n\u001b[1;32m    404\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _beam_search_sample(seq_groups, is_prompts,\n\u001b[1;32m    405\u001b[0m                                          sampling_metadata\u001b[38;5;241m.\u001b[39mseq_data,\n\u001b[1;32m    406\u001b[0m                                          beam_search_logprobs)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:235\u001b[0m, in \u001b[0;36m_random_sample\u001b[0;34m(selected_seq_groups, is_prompts, random_samples)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_random_sample\u001b[39m(\n\u001b[1;32m    230\u001b[0m     selected_seq_groups: List[Tuple[List[\u001b[38;5;28mint\u001b[39m], SamplingParams]],\n\u001b[1;32m    231\u001b[0m     is_prompts: List[\u001b[38;5;28mbool\u001b[39m],\n\u001b[1;32m    232\u001b[0m     random_samples: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    233\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# Find the maximum best_of value of the prompt phase requests.\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     random_samples \u001b[38;5;241m=\u001b[39m random_samples\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    236\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    237\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"告诉我有关上海的特产\",\n",
    "] * 100\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "num_tokens = sum([len(output.outputs[0].token_ids) for output in outputs]) \n",
    "spend_time = time.time() - start\n",
    "print(num_tokens / spend_time )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e6b49f7-8d4c-4060-9130-9f96f7dba4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1732"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  tp    1     2\n",
    "# 1     71     \n",
    "# 10    224\n",
    "# 100   894\n",
    "# 500   1825\n",
    "# 1000  2022   2042\n",
    "# 10000 2258\n",
    "\n",
    "# 张量并行数量增大会明显降低显存占用，但是不会增大推理速度，甚至略微降低\n",
    "#       chat  AWQ(1.5) GPTQ(1.5)  MyGPTQ    GPTQ(1)\n",
    "# 1     71      100    134     135\n",
    "# 10    224     324    360     473\n",
    "# 100   894     910    980     827           827           \n",
    "# 500   1825    1553   1526    858\n",
    "# 1000  2022    1642   1828\n",
    "# 10000 2258    1603   2159\n",
    "\n",
    "# batch size 小于100时，推理速度受限于内存中传输权重的带宽，量化模型较小所以可以更快\n",
    "# batch size 大时，推理速度受限于计算，量化模型的反量化操作增加了计算资源，推理速度更慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327b257-ef84-45a0-96ac-14ac254347c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76357bdb-f280-4928-874b-e8096e9b2083",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "str = '北京的特产有“北京烤鸭”、“豆汁焦圈”、“羊蝎子”和“冰糖葫芦”等，其中“北京烤鸭”是最具代表性的，它以其独特的烹饪技术和美味而闻名；“豆汁焦圈”是一种传统的北京小吃，豆汁是用绿豆发酵而成，焦圈是用面粉和糖制成；“羊蝎子”是一种独特的烹饪方法，它使用羊的脊髓，经过烹饪后变得鲜美可口；“冰糖葫芦”是一种以山楂、葡萄、苹果等水果串起来，外面包裹冰糖的特色小吃，口感酸甜可口。'\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    '/data/liucd/langchain/Qwen/Qwen1___5-7B-Chat-AWQ', # path to the output directory\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "out = tokenizer(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5667e-afb1-40a3-aabf-bb9257f3950f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "260174bf-3a6d-43cb-9a5a-adc65bf262d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16a0b5d5-0715-4f56-88d8-5dcd90077e40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [68990, 9370, 112984, 18830, 2073, 68990, 102554, 105397, 854, 91956, 99955, 102514, 100479, 100044, 854, 91956, 101187, 116558, 44729, 854, 33108, 2073, 100038, 100443, 111065, 854, 49567, 3837, 90919, 2073, 68990, 102554, 105397, 854, 104890, 76813, 99661, 104196, 3837, 99652, 109859, 105071, 111611, 107772, 106800, 68536, 109836, 24968, 2073, 99955, 102514, 100479, 100044, 854, 101158, 105062, 68990, 107600, 3837, 99955, 102514, 20412, 11622, 118693, 108279, 106042, 3837, 100479, 100044, 20412, 11622, 112398, 33108, 100443, 108711, 24968, 2073, 101187, 116558, 44729, 854, 101158, 105071, 111611, 39907, 3837, 99652, 37029, 101187, 9370, 109257, 103661, 3837, 101897, 111611, 33447, 101197, 99705, 57566, 30440, 39426, 24968, 2073, 100038, 100443, 111065, 854, 101158, 23031, 57811, 121142, 5373, 101580, 5373, 104167, 49567, 104618, 51575, 99793, 3837, 102586, 108232, 100038, 100443, 9370, 100175, 107600, 3837, 107816, 99918, 100475, 30440, 39426, 1773], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5eb5c05-3b27-4e0a-b06f-74d293d11610",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched '}' (3344056460.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[57], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    }'\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched '}'\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:1212/v1/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "\"model\": \"facebook/opt-125m\",\n",
    "\"prompt\": \"San Francisco is a\",\n",
    "\"max_tokens\": 7,\n",
    "\"temperature\": 0\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae81bd6-64dc-4d42-b963-af63070c41ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
