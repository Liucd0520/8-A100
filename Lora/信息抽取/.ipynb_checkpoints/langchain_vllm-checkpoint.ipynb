{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0888c38-b8f7-4f58-9b27-41eb44586c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 09:34:48,385 - modelscope - WARNING - Model revision not specified, use revision: v1.0.8\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 8.21k/8.21k [00:00<00:00, 11.7MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 50.8k/50.8k [00:00<00:00, 689kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1.17k/1.17k [00:00<00:00, 3.86MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 76.0/76.0 [00:00<00:00, 287kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2.29k/2.29k [00:00<00:00, 9.92MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1.88k/1.88k [00:00<00:00, 7.83MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 249/249 [00:00<00:00, 890kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 6.73k/6.73k [00:00<00:00, 18.1MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████▉| 1.91G/1.91G [00:19<00:00, 105MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████▉| 1.89G/1.89G [00:31<00:00, 63.9MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████▉| 1.90G/1.90G [00:21<00:00, 96.8MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████▉| 1.87G/1.87G [00:27<00:00, 73.1MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████▉| 1.45G/1.45G [00:17<00:00, 89.8MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 80.2k/80.2k [00:00<00:00, 1.38MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 54.5k/54.5k [00:00<00:00, 934kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2.64k/2.64k [00:00<00:00, 11.7MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 214/214 [00:00<00:00, 960kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2.44M/2.44M [00:00<00:00, 11.0MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 14.3k/14.3k [00:00<00:00, 481kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 27.2k/27.2k [00:00<00:00, 841kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 9.39k/9.39k [00:00<00:00, 14.7MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 173/173 [00:00<00:00, 766kB/s]\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "# Downloading model checkpoint to a local dir model_dir\n",
    "model_dir = snapshot_download('Qwen/Qwen-14B-Chat-Int4', cache_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8d729-4361-45de-abad-1f8699fe148c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b0f7c0b-c954-4d20-8562-c225e63d77c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-09 09:44:40 config.py:211] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 04-09 09:44:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='Qwen/Qwen1___5-14B-Chat-AWQ/', tokenizer='Qwen/Qwen1___5-14B-Chat-AWQ/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 09:44:43 model_runner.py:104] Loading model weights took 9.0594 GB\n",
      "INFO 04-09 09:44:48 gpu_executor.py:94] # GPU blocks: 839, # CPU blocks: 327\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model's max seq len (32768) is larger than the maximum number of tokens that can be stored in KV cache (13424). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[1;32m      2\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, my name is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe president of the United States is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe capital of France is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe future of AI is\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m llm \u001b[38;5;241m=\u001b[39m LLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen1___5-14B-Chat-AWQ/\u001b[39m\u001b[38;5;124m\"\u001b[39m, gpu_memory_utilization\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/llm.py:112\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     94\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m     95\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     96\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    111\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m LLMEngine\u001b[38;5;241m.\u001b[39mfrom_engine_args(\n\u001b[1;32m    113\u001b[0m     engine_args, usage_context\u001b[38;5;241m=\u001b[39mUsageContext\u001b[38;5;241m.\u001b[39mLLM_CLASS)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/llm_engine.py:196\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    193\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;241m*\u001b[39mengine_configs,\n\u001b[1;32m    198\u001b[0m     executor_class\u001b[38;5;241m=\u001b[39mexecutor_class,\n\u001b[1;32m    199\u001b[0m     log_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m engine_args\u001b[38;5;241m.\u001b[39mdisable_log_stats,\n\u001b[1;32m    200\u001b[0m     usage_context\u001b[38;5;241m=\u001b[39musage_context,\n\u001b[1;32m    201\u001b[0m )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/llm_engine.py:110\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config, executor_class, log_stats, usage_context)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenizer \u001b[38;5;241m=\u001b[39m Detokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(model_config, cache_config,\n\u001b[1;32m    111\u001b[0m                                      parallel_config, scheduler_config,\n\u001b[1;32m    112\u001b[0m                                      device_config, lora_config,\n\u001b[1;32m    113\u001b[0m                                      vision_language_config)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:40\u001b[0m, in \u001b[0;36mGPUExecutor.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, lora_config, vision_language_config)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_worker()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_cache()\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:97\u001b[0m, in \u001b[0;36mGPUExecutor._init_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m     num_gpu_blocks \u001b[38;5;241m=\u001b[39m forced_num_gpu_blocks\n\u001b[1;32m     94\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# GPU blocks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_gpu_blocks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# CPU blocks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cpu_blocks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m check_block_size_valid(num_gpu_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mblock_size,\n\u001b[1;32m     98\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mmax_model_len)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/vllm/executor/utils.py:8\u001b[0m, in \u001b[0;36mcheck_block_size_valid\u001b[0;34m(num_gpu_blocks, block_size, max_model_len)\u001b[0m\n\u001b[1;32m      6\u001b[0m max_seq_len \u001b[38;5;241m=\u001b[39m block_size \u001b[38;5;241m*\u001b[39m num_gpu_blocks\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_model_len \u001b[38;5;241m>\u001b[39m max_seq_len:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms max seq len (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_model_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis larger than the maximum number of tokens that can be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstored in KV cache (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_seq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Try increasing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`gpu_memory_utilization` or decreasing `max_model_len` when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitializing the engine.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The model's max seq len (32768) is larger than the maximum number of tokens that can be stored in KV cache (13424). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine."
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "llm = LLM(model=\"Qwen/Qwen1___5-14B-Chat-AWQ/\", gpu_memory_utilization=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "876aaea4-796a-4903-ac52-ff9baf18a807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wang Lei and I am from China. I am a 14-year-old\n",
      " the head of state and government of the United States. The president leads the executive\n",
      "____．（　　）\n",
      "A. Berlin\n",
      "B. Rome\n",
      "C.\n",
      " exciting， but it also raises important questions about the impact on jobs and society.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b900e0-5f64-4f1f-883a-8a5d56fc7fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-08 15:20:30 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='Qwen/Qwen1___5-7B-Chat/', tokenizer='Qwen/Qwen1___5-7B-Chat/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-08 15:20:32 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance.\n",
      "INFO 04-08 15:20:32 selector.py:25] Using XFormers backend.\n",
      "INFO 04-08 15:20:39 model_runner.py:104] Loading model weights took 14.3919 GB\n",
      "INFO 04-08 15:20:42 gpu_executor.py:94] # GPU blocks: 2264, # CPU blocks: 512\n",
      "INFO 04-08 15:20:44 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-08 15:20:44 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-08 15:20:49 model_runner.py:867] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "import os\n",
    "\n",
    "llm = VLLM(\n",
    "    model=\"Qwen/Qwen1___5-7B-Chat/\",\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=2560,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1e022-7b8d-446c-b4e7-29d2b90c90cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e60894-5141-44aa-96bd-705115a8336b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "417fc84a-f0e5-4a45-8cd3-b884d0c2ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-30 14:06:01 api_server.py:149] vLLM API server version 0.4.0.post1\n",
      "INFO 04-30 14:06:01 api_server.py:150] args: Namespace(host=None, port=1122, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name='Qwen1.5-32B-Chat-GPTQ', lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='Qwen/Qwen-72B-Chat-Int4/', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=124, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.58, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization='gptq', enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\n",
      "WARNING 04-30 14:06:01 config.py:211] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "2024-04-30 14:06:03,870\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "INFO 04-30 14:06:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='Qwen/Qwen-72B-Chat-Int4/', tokenizer='Qwen/Qwen-72B-Chat-Int4/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=124, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "WARNING 04-30 14:06:26 tokenizer.py:104] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 04-30 14:06:34 selector.py:16] Using FlashAttention backend.\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m INFO 04-30 14:06:39 selector.py:16] Using FlashAttention backend.\n",
      "INFO 04-30 14:06:42 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m INFO 04-30 14:06:42 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44] Error executing method load_model. This might cause deadlock in distributed execution.\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44] Traceback (most recent call last):\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/ray_utils.py\", line 37, in execute_method\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     return executor(*args, **kwargs)\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/worker/worker.py\", line 107, in load_model\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     self.model_runner.load_model()\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 95, in load_model\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     self.model = get_model(\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]                  ^^^^^^^^^^\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/model_loader.py\", line 91, in get_model\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     model = model_class(model_config.hf_config, linear_method)\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py\", line 227, in __init__\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     self.transformer = QWenModel(config, linear_method)\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py\", line 189, in __init__\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     self.h = nn.ModuleList([\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]                            ^\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py\", line 190, in <listcomp>\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     QWenBlock(config, linear_method)\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py\", line 143, in __init__\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     self.mlp = QWenMLP(config.hidden_size,\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py\", line 43, in __init__\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     self.gate_up_proj = MergedColumnParallelLinear(\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 260, in __init__\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     super().__init__(input_size, sum(output_sizes), bias, gather_output,\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 181, in __init__\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     self.linear_weights = self.linear_method.create_weights(\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/gptq.py\", line 129, in create_weights\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     torch.empty(\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]   File \"/data/liucd/anaconda/lib/python3.11/site-packages/torch/utils/_device.py\", line 77, in __torch_function__\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]     return func(*args, **kwargs)\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RayWorkerVllm pid=463075)\u001b[0m ERROR 04-30 14:06:55 ray_utils.py:44] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 1 has a total capacty of 39.39 GiB of which 93.38 MiB is free. Process 447741 has 27.02 GiB memory in use. Process 457917 has 416.00 MiB memory in use. Including non-PyTorch memory, this process has 11.84 GiB memory in use. Of the allocated memory 11.20 GiB is allocated by PyTorch, and 16.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "INFO 04-30 14:07:13 model_runner.py:104] Loading model weights took 19.2528 GB\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 157, in <module>\n",
      "    engine = AsyncLLMEngine.from_engine_args(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 348, in from_engine_args\n",
      "    engine = cls(\n",
      "             ^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 311, in __init__\n",
      "    self.engine = self._init_engine(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 422, in _init_engine\n",
      "    return engine_class(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 110, in __init__\n",
      "    self.model_executor = executor_class(model_config, cache_config,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 62, in __init__\n",
      "    self._init_workers_ray(placement_group)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 192, in _init_workers_ray\n",
      "    self._run_workers(\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 340, in _run_workers\n",
      "    ray_worker_outputs = ray.get(ray_worker_outputs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::RayWorkerVllm.execute_method()\u001b[39m (pid=463075, ip=10.89.201.121, actor_id=15473ef14ca22d80435efd0401000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7ff2283f2f10>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/ray_utils.py\", line 45, in execute_method\n",
      "    raise e\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/ray_utils.py\", line 37, in execute_method\n",
      "    return executor(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/worker/worker.py\", line 107, in load_model\n",
      "    self.model_runner.load_model()\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 95, in load_model\n",
      "    self.model = get_model(\n",
      "                 ^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/model_loader.py\", line 91, in get_model\n",
      "    model = model_class(model_config.hf_config, linear_method)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py\", line 227, in __init__\n",
      "    self.transformer = QWenModel(config, linear_method)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py\", line 189, in __init__\n",
      "    self.h = nn.ModuleList([\n",
      "                           ^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py\", line 190, in <listcomp>\n",
      "    QWenBlock(config, linear_method)\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py\", line 143, in __init__\n",
      "    self.mlp = QWenMLP(config.hidden_size,\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/models/qwen.py\", line 43, in __init__\n",
      "    self.gate_up_proj = MergedColumnParallelLinear(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 260, in __init__\n",
      "    super().__init__(input_size, sum(output_sizes), bias, gather_output,\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 181, in __init__\n",
      "    self.linear_weights = self.linear_method.create_weights(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/gptq.py\", line 129, in create_weights\n",
      "    torch.empty(\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/torch/utils/_device.py\", line 77, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 1 has a total capacty of 39.39 GiB of which 93.38 MiB is free. Process 447741 has 27.02 GiB memory in use. Process 457917 has 416.00 MiB memory in use. Including non-PyTorch memory, this process has 11.84 GiB memory in use. Of the allocated memory 11.20 GiB is allocated by PyTorch, and 16.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[0m[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n"
     ]
    }
   ],
   "source": [
    "# 量化版本\n",
    "!python -m vllm.entrypoints.openai.api_server --served-model-name Qwen1.5-32B-Chat-GPTQ \\\n",
    "      --model Qwen/Qwen-72B-Chat-Int4/ --max-model-len 124 --port 1122 --quantization gptq \\\n",
    "      --gpu-memory-utilization 0.58 --tensor-parallel-size 2 --trust-remote-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7b6781-fd4a-4eba-8763-b129346d7961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: api_server.py [-h] [--host HOST] [--port PORT]\n",
      "                     [--uvicorn-log-level {debug,info,warning,error,critical,trace}]\n",
      "                     [--allow-credentials] [--allowed-origins ALLOWED_ORIGINS]\n",
      "                     [--allowed-methods ALLOWED_METHODS]\n",
      "                     [--allowed-headers ALLOWED_HEADERS] [--api-key API_KEY]\n",
      "                     [--served-model-name SERVED_MODEL_NAME]\n",
      "                     [--lora-modules LORA_MODULES [LORA_MODULES ...]]\n",
      "                     [--chat-template CHAT_TEMPLATE]\n",
      "                     [--response-role RESPONSE_ROLE]\n",
      "                     [--ssl-keyfile SSL_KEYFILE] [--ssl-certfile SSL_CERTFILE]\n",
      "                     [--ssl-ca-certs SSL_CA_CERTS]\n",
      "                     [--ssl-cert-reqs SSL_CERT_REQS] [--root-path ROOT_PATH]\n",
      "                     [--middleware MIDDLEWARE] [--model MODEL]\n",
      "                     [--tokenizer TOKENIZER] [--revision REVISION]\n",
      "                     [--code-revision CODE_REVISION]\n",
      "                     [--tokenizer-revision TOKENIZER_REVISION]\n",
      "                     [--tokenizer-mode {auto,slow}] [--trust-remote-code]\n",
      "                     [--download-dir DOWNLOAD_DIR]\n",
      "                     [--load-format {auto,pt,safetensors,npcache,dummy}]\n",
      "                     [--dtype {auto,half,float16,bfloat16,float,float32}]\n",
      "                     [--kv-cache-dtype {auto,fp8_e5m2}]\n",
      "                     [--max-model-len MAX_MODEL_LEN] [--worker-use-ray]\n",
      "                     [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]\n",
      "                     [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n",
      "                     [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]\n",
      "                     [--ray-workers-use-nsight] [--block-size {8,16,32,128}]\n",
      "                     [--enable-prefix-caching] [--use-v2-block-manager]\n",
      "                     [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS] [--seed SEED]\n",
      "                     [--swap-space SWAP_SPACE]\n",
      "                     [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]\n",
      "                     [--forced-num-gpu-blocks FORCED_NUM_GPU_BLOCKS]\n",
      "                     [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]\n",
      "                     [--max-num-seqs MAX_NUM_SEQS]\n",
      "                     [--max-logprobs MAX_LOGPROBS] [--disable-log-stats]\n",
      "                     [--quantization {awq,gptq,squeezellm,None}]\n",
      "                     [--enforce-eager]\n",
      "                     [--max-context-len-to-capture MAX_CONTEXT_LEN_TO_CAPTURE]\n",
      "                     [--disable-custom-all-reduce]\n",
      "                     [--tokenizer-pool-size TOKENIZER_POOL_SIZE]\n",
      "                     [--tokenizer-pool-type TOKENIZER_POOL_TYPE]\n",
      "                     [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]\n",
      "                     [--enable-lora] [--max-loras MAX_LORAS]\n",
      "                     [--max-lora-rank MAX_LORA_RANK]\n",
      "                     [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]\n",
      "                     [--lora-dtype {auto,float16,bfloat16,float32}]\n",
      "                     [--max-cpu-loras MAX_CPU_LORAS]\n",
      "                     [--device {auto,cuda,neuron,cpu}]\n",
      "                     [--image-input-type {pixel_values,image_features}]\n",
      "                     [--image-token-id IMAGE_TOKEN_ID]\n",
      "                     [--image-input-shape IMAGE_INPUT_SHAPE]\n",
      "                     [--image-feature-size IMAGE_FEATURE_SIZE]\n",
      "                     [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]\n",
      "                     [--enable-chunked-prefill ENABLE_CHUNKED_PREFILL]\n",
      "                     [--engine-use-ray] [--disable-log-requests]\n",
      "                     [--max-log-len MAX_LOG_LEN]\n",
      "api_server.py: error: unrecognized arguments: --served-model-nme Qwen1.5-14B-Chat\n"
     ]
    }
   ],
   "source": [
    "# 非量化版本: 7B 模型\n",
    "!python -m vllm.entrypoints.openai.api_server --served-model-name Qwen1.5-7B-Chat \\\n",
    "      --model Qwen/Qwen1___5-14B-Chat  --port 1122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46159302-0946-4e34-99af-2eda83da6ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[1;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection.py:122\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 122\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    123\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_backends/sync.py:205\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    203\u001b[0m }\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    206\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    207\u001b[0m         address,\n\u001b[1;32m    208\u001b[0m         timeout,\n\u001b[1;32m    209\u001b[0m         source_address\u001b[38;5;241m=\u001b[39msource_address,\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:898\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 898\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    899\u001b[0m         request,\n\u001b[1;32m    900\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m    901\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    902\u001b[0m     )\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:232\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m openai_api_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:1122/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      7\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_api_key,\n\u001b[1;32m      8\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mopenai_api_base,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen1.5-7B-Chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     14\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你是一个人工智能助手.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     15\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m告诉我有关北京的特产\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     16\u001b[0m     ]\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_response)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    661\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    662\u001b[0m             {\n\u001b[1;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    683\u001b[0m             },\n\u001b[1;32m    684\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    685\u001b[0m         ),\n\u001b[1;32m    686\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    687\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    688\u001b[0m         ),\n\u001b[1;32m    689\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    690\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    692\u001b[0m     )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:1180\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1177\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1178\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1179\u001b[0m     )\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:869\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    862\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    870\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    871\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    872\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    873\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    874\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    875\u001b[0m     )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    923\u001b[0m         options,\n\u001b[1;32m    924\u001b[0m         cast_to,\n\u001b[1;32m    925\u001b[0m         retries,\n\u001b[1;32m    926\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    927\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    928\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    994\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    995\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    996\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m    997\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    998\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    999\u001b[0m )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    923\u001b[0m         options,\n\u001b[1;32m    924\u001b[0m         cast_to,\n\u001b[1;32m    925\u001b[0m         retries,\n\u001b[1;32m    926\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    927\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    928\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    994\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    995\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    996\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m    997\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    998\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    999\u001b[0m )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:932\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    923\u001b[0m             options,\n\u001b[1;32m    924\u001b[0m             cast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m             response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    929\u001b[0m         )\n\u001b[1;32m    931\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 932\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    934\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl, response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mreason_phrase\n\u001b[1;32m    936\u001b[0m )\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:1122/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"Qwen1.5-7B-Chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个人工智能助手.\"},\n",
    "        {\"role\": \"user\", \"content\": \"告诉我有关北京的特产\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82b5cb-c305-4537-867b-045b33110486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
