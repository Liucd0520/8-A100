{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8d729-4361-45de-abad-1f8699fe148c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1e022-7b8d-446c-b4e7-29d2b90c90cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e60894-5141-44aa-96bd-705115a8336b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "417fc84a-f0e5-4a45-8cd3-b884d0c2ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-16 13:54:59 api_server.py:151] vLLM API server version 0.4.1\n",
      "INFO 05-16 13:54:59 api_server.py:152] args: Namespace(host=None, port=1122, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=['quant_lora'], lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='merge_lora', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_max_model_len=None, model_loader_extra_config=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\n",
      "INFO 05-16 13:54:59 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='merge_lora', speculative_config=None, tokenizer='merge_lora', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n",
      "WARNING 05-16 13:54:59 tokenizer.py:123] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 05-16 13:54:59 utils.py:608] Found nccl from library /home/liuchangdong/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-16 13:55:01 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-16 13:55:01 selector.py:33] Using XFormers backend.\n",
      "INFO 05-16 13:55:10 model_runner.py:173] Loading model weights took 14.3831 GB\n",
      "INFO 05-16 13:55:13 gpu_executor.py:119] # GPU blocks: 2449, # CPU blocks: 512\n",
      "INFO 05-16 13:55:19 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-16 13:55:19 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-16 13:55:33 model_runner.py:1057] Graph capturing finished in 14 secs.\n",
      "WARNING 05-16 13:55:33 tokenizer.py:123] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "WARNING 05-16 13:55:33 serving_chat.py:347] No chat template provided. Chat API will not work.\n",
      "WARNING 05-16 13:55:34 tokenizer.py:123] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m2908491\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:1122\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 05-16 13:55:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "INFO 05-16 13:55:51 async_llm_engine.py:524] Received request cmpl-f02b8bbfe2e64dc9a0894c70819cd701: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:51 metrics.py:229] Avg prompt throughput: 11.8 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:55:52 async_llm_engine.py:120] Finished request cmpl-f02b8bbfe2e64dc9a0894c70819cd701.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:52 async_llm_engine.py:524] Received request cmpl-ea5e6f5a97c34df5a5314d3e292a5ad6: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:52 async_llm_engine.py:120] Finished request cmpl-ea5e6f5a97c34df5a5314d3e292a5ad6.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:52 async_llm_engine.py:524] Received request cmpl-0d2b74232d7e4eddb0b7a6e4855c6a46: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:52 async_llm_engine.py:120] Finished request cmpl-0d2b74232d7e4eddb0b7a6e4855c6a46.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:52 async_llm_engine.py:524] Received request cmpl-f6d5a9f2da204b99ae2ed66fc3c189c0: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:53 async_llm_engine.py:120] Finished request cmpl-f6d5a9f2da204b99ae2ed66fc3c189c0.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:53 async_llm_engine.py:524] Received request cmpl-adf6744699ef42f4b18461ade53890e8: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:53 async_llm_engine.py:120] Finished request cmpl-adf6744699ef42f4b18461ade53890e8.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:53 async_llm_engine.py:524] Received request cmpl-704d0ea4660143ecae733fc002bc5d40: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:53 async_llm_engine.py:120] Finished request cmpl-704d0ea4660143ecae733fc002bc5d40.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:53 async_llm_engine.py:524] Received request cmpl-8381d55faf914d1281a4f442213bd7b0: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:53 async_llm_engine.py:120] Finished request cmpl-8381d55faf914d1281a4f442213bd7b0.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:53 async_llm_engine.py:524] Received request cmpl-39977e959b6b4342b07d0a8caa20fb3a: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:54 async_llm_engine.py:120] Finished request cmpl-39977e959b6b4342b07d0a8caa20fb3a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:54 async_llm_engine.py:524] Received request cmpl-f4625669ff654dc68a5bf2da53a0db9d: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:54 async_llm_engine.py:120] Finished request cmpl-f4625669ff654dc68a5bf2da53a0db9d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:54 async_llm_engine.py:524] Received request cmpl-63bcf2ede054496da9b27f5cfee736cf: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:54 async_llm_engine.py:120] Finished request cmpl-63bcf2ede054496da9b27f5cfee736cf.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:54 async_llm_engine.py:524] Received request cmpl-6c6aea7d2cd34fbebb8f011f773f4cfd: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:55 async_llm_engine.py:120] Finished request cmpl-6c6aea7d2cd34fbebb8f011f773f4cfd.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:55 async_llm_engine.py:524] Received request cmpl-e48b638d97e44e4ebfbc7f662b483d84: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:55 async_llm_engine.py:120] Finished request cmpl-e48b638d97e44e4ebfbc7f662b483d84.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:55 async_llm_engine.py:524] Received request cmpl-66ffb5f059d549f68f2c5a2e5c890e8d: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:55 async_llm_engine.py:120] Finished request cmpl-66ffb5f059d549f68f2c5a2e5c890e8d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:55 async_llm_engine.py:524] Received request cmpl-e75e75e511ce49caac28cbbd40e33499: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:56 async_llm_engine.py:120] Finished request cmpl-e75e75e511ce49caac28cbbd40e33499.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:56 async_llm_engine.py:524] Received request cmpl-1c8348e4f34e4d79ae1acbc8b7d77db3: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:56 async_llm_engine.py:120] Finished request cmpl-1c8348e4f34e4d79ae1acbc8b7d77db3.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:56 async_llm_engine.py:524] Received request cmpl-3e1342fd2c86423da6ae0c376ed578f1: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:56 async_llm_engine.py:120] Finished request cmpl-3e1342fd2c86423da6ae0c376ed578f1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:56 async_llm_engine.py:524] Received request cmpl-86b561a653bc4c02aa329a5cadc613f6: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:56 metrics.py:229] Avg prompt throughput: 283.4 tokens/s, Avg generation throughput: 64.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:55:57 async_llm_engine.py:120] Finished request cmpl-86b561a653bc4c02aa329a5cadc613f6.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:57 async_llm_engine.py:524] Received request cmpl-09784ebb9cf94c00ada59e9635a22ede: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:57 async_llm_engine.py:120] Finished request cmpl-09784ebb9cf94c00ada59e9635a22ede.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:57 async_llm_engine.py:524] Received request cmpl-5c68ca8195f645058dae9dc5d648ccce: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:57 async_llm_engine.py:120] Finished request cmpl-5c68ca8195f645058dae9dc5d648ccce.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:57 async_llm_engine.py:524] Received request cmpl-24fc624ac0ad4b3f86602a266905b3b1: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:57 async_llm_engine.py:120] Finished request cmpl-24fc624ac0ad4b3f86602a266905b3b1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:57 async_llm_engine.py:524] Received request cmpl-f9a7a284e33347718dd03c04b69e9e2f: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:58 async_llm_engine.py:120] Finished request cmpl-f9a7a284e33347718dd03c04b69e9e2f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:58 async_llm_engine.py:524] Received request cmpl-68380e7fd0784d8ba76188fb4721a275: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:58 async_llm_engine.py:120] Finished request cmpl-68380e7fd0784d8ba76188fb4721a275.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:58 async_llm_engine.py:524] Received request cmpl-bf5c5afaff474d049c4771a8c0bf40b8: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:58 async_llm_engine.py:120] Finished request cmpl-bf5c5afaff474d049c4771a8c0bf40b8.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:58 async_llm_engine.py:524] Received request cmpl-18d60964c62a4b88a5a6a224eb9fc7e6: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:59 async_llm_engine.py:120] Finished request cmpl-18d60964c62a4b88a5a6a224eb9fc7e6.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:59 async_llm_engine.py:524] Received request cmpl-1e09d5e0c19142ad95a3e007cbf98134: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:59 async_llm_engine.py:120] Finished request cmpl-1e09d5e0c19142ad95a3e007cbf98134.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:59 async_llm_engine.py:524] Received request cmpl-f2d38aae85784a6897a673954896e7be: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:55:59 async_llm_engine.py:120] Finished request cmpl-f2d38aae85784a6897a673954896e7be.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:55:59 async_llm_engine.py:524] Received request cmpl-4f51d27d309c458d968b3e2a2e1b4443: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:00 async_llm_engine.py:120] Finished request cmpl-4f51d27d309c458d968b3e2a2e1b4443.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:00 async_llm_engine.py:524] Received request cmpl-d67ed54763624d9cb6b4efd08d288298: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:00 async_llm_engine.py:120] Finished request cmpl-d67ed54763624d9cb6b4efd08d288298.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:00 async_llm_engine.py:524] Received request cmpl-7c0dbeaafe384969b29b3e81da30c8c3: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:00 async_llm_engine.py:120] Finished request cmpl-7c0dbeaafe384969b29b3e81da30c8c3.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:00 async_llm_engine.py:524] Received request cmpl-bc741ec841e34e6f91aad515d06898fc: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:00 async_llm_engine.py:120] Finished request cmpl-bc741ec841e34e6f91aad515d06898fc.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:01 async_llm_engine.py:524] Received request cmpl-17b7507643d54c808ba5a03e81d9c398: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:01 async_llm_engine.py:120] Finished request cmpl-17b7507643d54c808ba5a03e81d9c398.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:01 async_llm_engine.py:524] Received request cmpl-0e48a13471fd4ce7a84bd9a34eeff327: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:01 async_llm_engine.py:120] Finished request cmpl-0e48a13471fd4ce7a84bd9a34eeff327.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:01 async_llm_engine.py:524] Received request cmpl-8fdae346001f4bf582d774ee671ca6a6: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:01 metrics.py:229] Avg prompt throughput: 283.2 tokens/s, Avg generation throughput: 65.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:56:01 async_llm_engine.py:120] Finished request cmpl-8fdae346001f4bf582d774ee671ca6a6.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:01 async_llm_engine.py:524] Received request cmpl-b27f07d5b0aa47c4b1bdf8b72997b998: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:02 async_llm_engine.py:120] Finished request cmpl-b27f07d5b0aa47c4b1bdf8b72997b998.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:02 async_llm_engine.py:524] Received request cmpl-0f8a7a86a3594579b6eb8cb6d3dc0d9e: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:02 async_llm_engine.py:120] Finished request cmpl-0f8a7a86a3594579b6eb8cb6d3dc0d9e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:02 async_llm_engine.py:524] Received request cmpl-d8d15e06315a4585bcd0a35358683292: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:03 async_llm_engine.py:120] Finished request cmpl-d8d15e06315a4585bcd0a35358683292.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:03 async_llm_engine.py:524] Received request cmpl-dfd3b1b8422343f588fbc714bbb46c3b: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:03 async_llm_engine.py:120] Finished request cmpl-dfd3b1b8422343f588fbc714bbb46c3b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:03 async_llm_engine.py:524] Received request cmpl-f0eec19777be4781a64c10974b6929e2: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:03 async_llm_engine.py:120] Finished request cmpl-f0eec19777be4781a64c10974b6929e2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:03 async_llm_engine.py:524] Received request cmpl-22738781868c4c1f8985fe9662db154a: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:03 async_llm_engine.py:120] Finished request cmpl-22738781868c4c1f8985fe9662db154a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:03 async_llm_engine.py:524] Received request cmpl-0f02c8aaac85456494a48a3007d3b989: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:04 async_llm_engine.py:120] Finished request cmpl-0f02c8aaac85456494a48a3007d3b989.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:04 async_llm_engine.py:524] Received request cmpl-42df2b4507c14825b6b3269b2fce5d16: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:04 async_llm_engine.py:120] Finished request cmpl-42df2b4507c14825b6b3269b2fce5d16.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:04 async_llm_engine.py:524] Received request cmpl-c5fcc7ae99054eee8e2f4a4855f52da4: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:04 async_llm_engine.py:120] Finished request cmpl-c5fcc7ae99054eee8e2f4a4855f52da4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:04 async_llm_engine.py:524] Received request cmpl-b90488b50d8b4a368c75df7741040c1b: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:05 async_llm_engine.py:120] Finished request cmpl-b90488b50d8b4a368c75df7741040c1b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:05 async_llm_engine.py:524] Received request cmpl-dcf84b06b9284c3f940082cc980eb060: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:05 async_llm_engine.py:120] Finished request cmpl-dcf84b06b9284c3f940082cc980eb060.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:05 async_llm_engine.py:524] Received request cmpl-29ad54f08451487a8e1a637bc0d1cac0: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:05 async_llm_engine.py:120] Finished request cmpl-29ad54f08451487a8e1a637bc0d1cac0.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:05 async_llm_engine.py:524] Received request cmpl-dd14081e32644994aaeab60a175b5a7b: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:06 async_llm_engine.py:120] Finished request cmpl-dd14081e32644994aaeab60a175b5a7b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:06 async_llm_engine.py:524] Received request cmpl-3b1e042c89e840718641c95f9692510d: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:06 async_llm_engine.py:120] Finished request cmpl-3b1e042c89e840718641c95f9692510d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:06 async_llm_engine.py:524] Received request cmpl-962f84aed9a44e1c83ed7f7538b4aa34: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:06 async_llm_engine.py:120] Finished request cmpl-962f84aed9a44e1c83ed7f7538b4aa34.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:06 async_llm_engine.py:524] Received request cmpl-312a3fbbf61044dca0596c1d78f0cbc0: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:06 metrics.py:229] Avg prompt throughput: 283.3 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:56:06 async_llm_engine.py:120] Finished request cmpl-312a3fbbf61044dca0596c1d78f0cbc0.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:06 async_llm_engine.py:524] Received request cmpl-247355e20fa54d509e15fc52151d9481: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:07 async_llm_engine.py:120] Finished request cmpl-247355e20fa54d509e15fc52151d9481.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:07 async_llm_engine.py:524] Received request cmpl-04af581c33914e3b95454c98307e9189: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:07 async_llm_engine.py:120] Finished request cmpl-04af581c33914e3b95454c98307e9189.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:07 async_llm_engine.py:524] Received request cmpl-5004e2a0ea0746ef9e68f4a74e9f531d: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:07 async_llm_engine.py:120] Finished request cmpl-5004e2a0ea0746ef9e68f4a74e9f531d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:07 async_llm_engine.py:524] Received request cmpl-52a49eb51a2c4a169c5f1a6b996bbdd1: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:08 async_llm_engine.py:120] Finished request cmpl-52a49eb51a2c4a169c5f1a6b996bbdd1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:08 async_llm_engine.py:524] Received request cmpl-2fc662171f904810b3043fcc835b26b7: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:08 async_llm_engine.py:120] Finished request cmpl-2fc662171f904810b3043fcc835b26b7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:08 async_llm_engine.py:524] Received request cmpl-0df99b7f4bc747fc9133afc65b383540: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:08 async_llm_engine.py:120] Finished request cmpl-0df99b7f4bc747fc9133afc65b383540.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:08 async_llm_engine.py:524] Received request cmpl-8551580b8cba476aa9d7fc3bbb8a2ca5: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:09 async_llm_engine.py:120] Finished request cmpl-8551580b8cba476aa9d7fc3bbb8a2ca5.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:09 async_llm_engine.py:524] Received request cmpl-6e40738219a84f479a2163c50f3cb574: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:09 async_llm_engine.py:120] Finished request cmpl-6e40738219a84f479a2163c50f3cb574.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:09 async_llm_engine.py:524] Received request cmpl-f97d4097759146ea97017698992abca7: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:09 async_llm_engine.py:120] Finished request cmpl-f97d4097759146ea97017698992abca7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:09 async_llm_engine.py:524] Received request cmpl-7c36ab355e2a4ce1a313b1e41e0e9cd8: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:09 async_llm_engine.py:120] Finished request cmpl-7c36ab355e2a4ce1a313b1e41e0e9cd8.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:09 async_llm_engine.py:524] Received request cmpl-20af4306b63e45ed82608743fa6edc53: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:10 async_llm_engine.py:120] Finished request cmpl-20af4306b63e45ed82608743fa6edc53.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:10 async_llm_engine.py:524] Received request cmpl-2d41b6bd125f4690b987c1257d59ae16: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:10 async_llm_engine.py:120] Finished request cmpl-2d41b6bd125f4690b987c1257d59ae16.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:10 async_llm_engine.py:524] Received request cmpl-8631af847edd42d093688cbeb13c0bb2: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:10 async_llm_engine.py:120] Finished request cmpl-8631af847edd42d093688cbeb13c0bb2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:10 async_llm_engine.py:524] Received request cmpl-b7eee4705415404a892e6b78663389e4: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:11 async_llm_engine.py:120] Finished request cmpl-b7eee4705415404a892e6b78663389e4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:11 async_llm_engine.py:524] Received request cmpl-b069f9a2baaf4293ac125d8bef7a9049: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:11 async_llm_engine.py:120] Finished request cmpl-b069f9a2baaf4293ac125d8bef7a9049.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:11 async_llm_engine.py:524] Received request cmpl-7d94f1865e4c44cfba767e13161e2128: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:11 async_llm_engine.py:120] Finished request cmpl-7d94f1865e4c44cfba767e13161e2128.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:11 async_llm_engine.py:524] Received request cmpl-19842e51c3f348cb9071320ed0d60366: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:11 metrics.py:229] Avg prompt throughput: 300.0 tokens/s, Avg generation throughput: 64.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:56:12 async_llm_engine.py:120] Finished request cmpl-19842e51c3f348cb9071320ed0d60366.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:12 async_llm_engine.py:524] Received request cmpl-05074b5a1f0548eead7ebee3d093a66c: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:12 async_llm_engine.py:120] Finished request cmpl-05074b5a1f0548eead7ebee3d093a66c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:12 async_llm_engine.py:524] Received request cmpl-82048405198b419a9e5b7ebc06438686: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:12 async_llm_engine.py:120] Finished request cmpl-82048405198b419a9e5b7ebc06438686.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:12 async_llm_engine.py:524] Received request cmpl-ebbbab6d2ff046b1ae7e034cf117393c: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:13 async_llm_engine.py:120] Finished request cmpl-ebbbab6d2ff046b1ae7e034cf117393c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:13 async_llm_engine.py:524] Received request cmpl-cfc09344584b4e0a9401429dbc41c8db: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:13 async_llm_engine.py:120] Finished request cmpl-cfc09344584b4e0a9401429dbc41c8db.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:13 async_llm_engine.py:524] Received request cmpl-7e80cf6d855d4248a28ab8e1d10eb34a: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:13 async_llm_engine.py:120] Finished request cmpl-7e80cf6d855d4248a28ab8e1d10eb34a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:13 async_llm_engine.py:524] Received request cmpl-ecad5510ca7e4101b62bde95db3e1e6d: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:13 async_llm_engine.py:120] Finished request cmpl-ecad5510ca7e4101b62bde95db3e1e6d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:13 async_llm_engine.py:524] Received request cmpl-215878f73f364607b01798a66816dcef: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:14 async_llm_engine.py:120] Finished request cmpl-215878f73f364607b01798a66816dcef.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:14 async_llm_engine.py:524] Received request cmpl-50603c99e8c744a786ee63b014a7fd3a: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:14 async_llm_engine.py:120] Finished request cmpl-50603c99e8c744a786ee63b014a7fd3a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:14 async_llm_engine.py:524] Received request cmpl-0d41399437a74c8a99210ff6194cc14a: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:14 async_llm_engine.py:120] Finished request cmpl-0d41399437a74c8a99210ff6194cc14a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:14 async_llm_engine.py:524] Received request cmpl-2bb6cd13163543689cc2d5eac5be4f1b: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:15 async_llm_engine.py:120] Finished request cmpl-2bb6cd13163543689cc2d5eac5be4f1b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:15 async_llm_engine.py:524] Received request cmpl-363b807b1ac94defae87796e041118ad: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:15 async_llm_engine.py:120] Finished request cmpl-363b807b1ac94defae87796e041118ad.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:15 async_llm_engine.py:524] Received request cmpl-c3562c6cfa6742fea398ff3c1ab89994: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:15 async_llm_engine.py:120] Finished request cmpl-c3562c6cfa6742fea398ff3c1ab89994.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:15 async_llm_engine.py:524] Received request cmpl-daea7fc308d544e6b2c63c3c7e339404: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:16 async_llm_engine.py:120] Finished request cmpl-daea7fc308d544e6b2c63c3c7e339404.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:16 async_llm_engine.py:524] Received request cmpl-58064544d80b486b9294cdc9d6f4900e: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:16 async_llm_engine.py:120] Finished request cmpl-58064544d80b486b9294cdc9d6f4900e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47202 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:22 async_llm_engine.py:524] Received request cmpl-edd415403c9b4b0c92958a00245c3dcf: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:22 metrics.py:229] Avg prompt throughput: 124.5 tokens/s, Avg generation throughput: 27.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:56:22 async_llm_engine.py:120] Finished request cmpl-edd415403c9b4b0c92958a00245c3dcf.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:22 async_llm_engine.py:524] Received request cmpl-813a9069663b49088391b1f1d7c53dda: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:23 async_llm_engine.py:120] Finished request cmpl-813a9069663b49088391b1f1d7c53dda.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:23 async_llm_engine.py:524] Received request cmpl-21db7147c0a3495aa2dfef275c9c9438: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:23 async_llm_engine.py:120] Finished request cmpl-21db7147c0a3495aa2dfef275c9c9438.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:23 async_llm_engine.py:524] Received request cmpl-68adbf7956bb40d5bb2d8eadb1a2b20a: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:23 async_llm_engine.py:120] Finished request cmpl-68adbf7956bb40d5bb2d8eadb1a2b20a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:23 async_llm_engine.py:524] Received request cmpl-55092c5647d24a34939d459430b66660: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:24 async_llm_engine.py:120] Finished request cmpl-55092c5647d24a34939d459430b66660.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:24 async_llm_engine.py:524] Received request cmpl-f7d0e0452ab441959a44a291a1916ad8: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:24 async_llm_engine.py:120] Finished request cmpl-f7d0e0452ab441959a44a291a1916ad8.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:24 async_llm_engine.py:524] Received request cmpl-f67eebba98a740cea2f23d7733807b49: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:24 async_llm_engine.py:120] Finished request cmpl-f67eebba98a740cea2f23d7733807b49.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:24 async_llm_engine.py:524] Received request cmpl-0fec97a84bb547e098e2135f9b3c76dc: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:25 async_llm_engine.py:120] Finished request cmpl-0fec97a84bb547e098e2135f9b3c76dc.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:25 async_llm_engine.py:524] Received request cmpl-a09c0db6129140e1959417fbb1b80355: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:25 async_llm_engine.py:120] Finished request cmpl-a09c0db6129140e1959417fbb1b80355.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:25 async_llm_engine.py:524] Received request cmpl-ba7861cd57a948eb8aab57d2423f4edc: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:25 async_llm_engine.py:120] Finished request cmpl-ba7861cd57a948eb8aab57d2423f4edc.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:25 async_llm_engine.py:524] Received request cmpl-c5e2cf50e9294770bd64c5fe277fb4ba: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:26 async_llm_engine.py:120] Finished request cmpl-c5e2cf50e9294770bd64c5fe277fb4ba.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:26 async_llm_engine.py:524] Received request cmpl-5d72583434864e1786c3bd954c340ad7: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:26 async_llm_engine.py:120] Finished request cmpl-5d72583434864e1786c3bd954c340ad7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:26 async_llm_engine.py:524] Received request cmpl-c9310c0a459a4cf78541808500131972: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:26 async_llm_engine.py:120] Finished request cmpl-c9310c0a459a4cf78541808500131972.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:26 async_llm_engine.py:524] Received request cmpl-a3170f32ce08450b9c00d6434ca21f25: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:27 async_llm_engine.py:120] Finished request cmpl-a3170f32ce08450b9c00d6434ca21f25.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:27 async_llm_engine.py:524] Received request cmpl-2218fcedd3e5450cbc5edbb7a2956f28: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:27 async_llm_engine.py:120] Finished request cmpl-2218fcedd3e5450cbc5edbb7a2956f28.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:27 async_llm_engine.py:524] Received request cmpl-73bb075bcf994a8bbec5b2ffee1c9a3e: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:27 async_llm_engine.py:120] Finished request cmpl-73bb075bcf994a8bbec5b2ffee1c9a3e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:27 async_llm_engine.py:524] Received request cmpl-7f587bb68be743a5bba2e479df57451e: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:27 metrics.py:229] Avg prompt throughput: 281.5 tokens/s, Avg generation throughput: 64.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:56:27 async_llm_engine.py:120] Finished request cmpl-7f587bb68be743a5bba2e479df57451e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:27 async_llm_engine.py:524] Received request cmpl-211bfefb67984c558a6d96e37ba3c7d2: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:28 async_llm_engine.py:120] Finished request cmpl-211bfefb67984c558a6d96e37ba3c7d2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:28 async_llm_engine.py:524] Received request cmpl-3ef87ceac62b48c69c212b894e075dc7: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:28 async_llm_engine.py:120] Finished request cmpl-3ef87ceac62b48c69c212b894e075dc7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:28 async_llm_engine.py:524] Received request cmpl-9de3aa1f577846aa8e99b7027bc97aa4: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:28 async_llm_engine.py:120] Finished request cmpl-9de3aa1f577846aa8e99b7027bc97aa4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:28 async_llm_engine.py:524] Received request cmpl-4a1c13dea28449d2ae12a067813c97b7: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:29 async_llm_engine.py:120] Finished request cmpl-4a1c13dea28449d2ae12a067813c97b7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:29 async_llm_engine.py:524] Received request cmpl-904d6db332af44b59631831fd2ee67ee: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:29 async_llm_engine.py:120] Finished request cmpl-904d6db332af44b59631831fd2ee67ee.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:29 async_llm_engine.py:524] Received request cmpl-9a741224dc77436a9c3366ab2f4d86bc: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:29 async_llm_engine.py:120] Finished request cmpl-9a741224dc77436a9c3366ab2f4d86bc.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:29 async_llm_engine.py:524] Received request cmpl-3dc70a9d0fbb433eabfab1244bc4d5db: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:30 async_llm_engine.py:120] Finished request cmpl-3dc70a9d0fbb433eabfab1244bc4d5db.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:30 async_llm_engine.py:524] Received request cmpl-d048a4c636dd4189bd0c38c49536c0ad: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:30 async_llm_engine.py:120] Finished request cmpl-d048a4c636dd4189bd0c38c49536c0ad.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:30 async_llm_engine.py:524] Received request cmpl-9892343d98b24e87901433f62f9561c2: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:30 async_llm_engine.py:120] Finished request cmpl-9892343d98b24e87901433f62f9561c2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:30 async_llm_engine.py:524] Received request cmpl-5cc005d50505405daedb8221d817aace: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:31 async_llm_engine.py:120] Finished request cmpl-5cc005d50505405daedb8221d817aace.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:31 async_llm_engine.py:524] Received request cmpl-3a1f4ba486e04f27b391f359b5efb896: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:31 async_llm_engine.py:120] Finished request cmpl-3a1f4ba486e04f27b391f359b5efb896.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:31 async_llm_engine.py:524] Received request cmpl-8d4feb94ef0f44cbaceda983660a766c: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:31 async_llm_engine.py:120] Finished request cmpl-8d4feb94ef0f44cbaceda983660a766c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:31 async_llm_engine.py:524] Received request cmpl-5a44d664fb7241bcbb9f861d572c4e03: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:31 async_llm_engine.py:120] Finished request cmpl-5a44d664fb7241bcbb9f861d572c4e03.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:31 async_llm_engine.py:524] Received request cmpl-284b1c214c9e4842a42da7c38ae3a6f1: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:32 async_llm_engine.py:120] Finished request cmpl-284b1c214c9e4842a42da7c38ae3a6f1.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:32 async_llm_engine.py:524] Received request cmpl-4225e4c6586f4eb8a96d5fa57e0ef7e5: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:32 async_llm_engine.py:120] Finished request cmpl-4225e4c6586f4eb8a96d5fa57e0ef7e5.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:32 async_llm_engine.py:524] Received request cmpl-c5f0a85a4426432e8db91f38fb5ec5b2: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:32 metrics.py:229] Avg prompt throughput: 283.4 tokens/s, Avg generation throughput: 65.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:56:32 async_llm_engine.py:120] Finished request cmpl-c5f0a85a4426432e8db91f38fb5ec5b2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:32 async_llm_engine.py:524] Received request cmpl-c3ffb2de102c400f804d5194913d597b: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:33 async_llm_engine.py:120] Finished request cmpl-c3ffb2de102c400f804d5194913d597b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:33 async_llm_engine.py:524] Received request cmpl-8f91f13594b64f5580c0eb2476c160b5: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:33 async_llm_engine.py:120] Finished request cmpl-8f91f13594b64f5580c0eb2476c160b5.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:33 async_llm_engine.py:524] Received request cmpl-e98a695772c64ab3bd081e05a7898631: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:33 async_llm_engine.py:120] Finished request cmpl-e98a695772c64ab3bd081e05a7898631.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:33 async_llm_engine.py:524] Received request cmpl-6142e67bbb4049e299ddc3f0fa37fadf: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:34 async_llm_engine.py:120] Finished request cmpl-6142e67bbb4049e299ddc3f0fa37fadf.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:34 async_llm_engine.py:524] Received request cmpl-fc2d918cf72948f3bea9b3b968b205f2: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:34 async_llm_engine.py:120] Finished request cmpl-fc2d918cf72948f3bea9b3b968b205f2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:34 async_llm_engine.py:524] Received request cmpl-2d7d1acc5fc14bfc89748fe9df70a53c: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:34 async_llm_engine.py:120] Finished request cmpl-2d7d1acc5fc14bfc89748fe9df70a53c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:34 async_llm_engine.py:524] Received request cmpl-af606a73f3a14122a33395a09e3e7dfc: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:35 async_llm_engine.py:120] Finished request cmpl-af606a73f3a14122a33395a09e3e7dfc.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:35 async_llm_engine.py:524] Received request cmpl-af489affdc774964a313b4ae13dfd32e: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:35 async_llm_engine.py:120] Finished request cmpl-af489affdc774964a313b4ae13dfd32e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:35 async_llm_engine.py:524] Received request cmpl-b17bb7af32024a21bf185997c54244b4: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:35 async_llm_engine.py:120] Finished request cmpl-b17bb7af32024a21bf185997c54244b4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:35 async_llm_engine.py:524] Received request cmpl-fdec5a86a8d74e9d90daf768fad95f08: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:36 async_llm_engine.py:120] Finished request cmpl-fdec5a86a8d74e9d90daf768fad95f08.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:36 async_llm_engine.py:524] Received request cmpl-779a9c88148a451dbfaa0e1f347d7cfb: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:36 async_llm_engine.py:120] Finished request cmpl-779a9c88148a451dbfaa0e1f347d7cfb.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:36 async_llm_engine.py:524] Received request cmpl-89da399cade5480499f7a0d813cdabe4: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:36 async_llm_engine.py:120] Finished request cmpl-89da399cade5480499f7a0d813cdabe4.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:36 async_llm_engine.py:524] Received request cmpl-5fd5b1968773454dad702ac15d8ef92b: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:37 async_llm_engine.py:120] Finished request cmpl-5fd5b1968773454dad702ac15d8ef92b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:37 async_llm_engine.py:524] Received request cmpl-52390c971af5443299ef4e2ff277dca5: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:37 async_llm_engine.py:120] Finished request cmpl-52390c971af5443299ef4e2ff277dca5.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:37 async_llm_engine.py:524] Received request cmpl-928b083cf2d643ada713f710097edf26: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:37 async_llm_engine.py:120] Finished request cmpl-928b083cf2d643ada713f710097edf26.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:37 async_llm_engine.py:524] Received request cmpl-c26c398b1cd9426c827f4cf3c2493e45: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:37 metrics.py:229] Avg prompt throughput: 281.9 tokens/s, Avg generation throughput: 65.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:56:37 async_llm_engine.py:120] Finished request cmpl-c26c398b1cd9426c827f4cf3c2493e45.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:37 async_llm_engine.py:524] Received request cmpl-d978370d3d0b427886beef3fe8089f13: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:38 async_llm_engine.py:120] Finished request cmpl-d978370d3d0b427886beef3fe8089f13.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:38 async_llm_engine.py:524] Received request cmpl-b4a05ddc7b8840ac825dfae0226fe230: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:38 async_llm_engine.py:120] Finished request cmpl-b4a05ddc7b8840ac825dfae0226fe230.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:38 async_llm_engine.py:524] Received request cmpl-a2177728848a4b7e87eda541642ff941: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:38 async_llm_engine.py:120] Finished request cmpl-a2177728848a4b7e87eda541642ff941.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:38 async_llm_engine.py:524] Received request cmpl-b985c745b0fb4cfeaeea4f22264d8027: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:39 async_llm_engine.py:120] Finished request cmpl-b985c745b0fb4cfeaeea4f22264d8027.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:39 async_llm_engine.py:524] Received request cmpl-6b046aaa921a4aa7be74be6be6ecbde3: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:39 async_llm_engine.py:120] Finished request cmpl-6b046aaa921a4aa7be74be6be6ecbde3.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:39 async_llm_engine.py:524] Received request cmpl-6fbc1b64c46a4f37a5542d7cf70a9e35: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:39 async_llm_engine.py:120] Finished request cmpl-6fbc1b64c46a4f37a5542d7cf70a9e35.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:39 async_llm_engine.py:524] Received request cmpl-66396770cb1c4781bf858535c5643c81: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:40 async_llm_engine.py:120] Finished request cmpl-66396770cb1c4781bf858535c5643c81.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:40 async_llm_engine.py:524] Received request cmpl-ec3dc06791744a66b227e8bd01de0871: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:40 async_llm_engine.py:120] Finished request cmpl-ec3dc06791744a66b227e8bd01de0871.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:40 async_llm_engine.py:524] Received request cmpl-c7e53310a0424c7fbee116124c6e880f: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:40 async_llm_engine.py:120] Finished request cmpl-c7e53310a0424c7fbee116124c6e880f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:40 async_llm_engine.py:524] Received request cmpl-edfac79a994843ec87f7ff8c8563fe47: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:41 async_llm_engine.py:120] Finished request cmpl-edfac79a994843ec87f7ff8c8563fe47.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:41 async_llm_engine.py:524] Received request cmpl-08d24ba563b246ef8cf602d90a4d4067: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:41 async_llm_engine.py:120] Finished request cmpl-08d24ba563b246ef8cf602d90a4d4067.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:41 async_llm_engine.py:524] Received request cmpl-8309b29bbb9b4cc5acfccecf06a959bf: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:41 async_llm_engine.py:120] Finished request cmpl-8309b29bbb9b4cc5acfccecf06a959bf.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:41 async_llm_engine.py:524] Received request cmpl-4cac269a1dbd40f1990abe17771cb026: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:42 async_llm_engine.py:120] Finished request cmpl-4cac269a1dbd40f1990abe17771cb026.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:42 async_llm_engine.py:524] Received request cmpl-32a55b00094c46ee9a67d67cb587818b: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:42 async_llm_engine.py:120] Finished request cmpl-32a55b00094c46ee9a67d67cb587818b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:42 async_llm_engine.py:524] Received request cmpl-9a33eb77b45145f4a6136f8f138ddff5: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:42 metrics.py:229] Avg prompt throughput: 265.3 tokens/s, Avg generation throughput: 65.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:56:42 async_llm_engine.py:120] Finished request cmpl-9a33eb77b45145f4a6136f8f138ddff5.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:42 async_llm_engine.py:524] Received request cmpl-c193aea220d043bc8b947edfed4760d2: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:43 async_llm_engine.py:120] Finished request cmpl-c193aea220d043bc8b947edfed4760d2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:43 async_llm_engine.py:524] Received request cmpl-b7917831dc8e4858b9d0857fbc853635: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:43 async_llm_engine.py:120] Finished request cmpl-b7917831dc8e4858b9d0857fbc853635.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:43 async_llm_engine.py:524] Received request cmpl-cbb45db966a34461aebd1236b90f4ed7: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:43 async_llm_engine.py:120] Finished request cmpl-cbb45db966a34461aebd1236b90f4ed7.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:43 async_llm_engine.py:524] Received request cmpl-9135dfe22a0948a3b8edb52dcd3e1e6d: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:43 async_llm_engine.py:120] Finished request cmpl-9135dfe22a0948a3b8edb52dcd3e1e6d.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:43 async_llm_engine.py:524] Received request cmpl-a9794e343e19416b97b2545f4b0e895e: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:44 async_llm_engine.py:120] Finished request cmpl-a9794e343e19416b97b2545f4b0e895e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:44 async_llm_engine.py:524] Received request cmpl-d14adbf7dc12486baefab2996b01f97a: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:44 async_llm_engine.py:120] Finished request cmpl-d14adbf7dc12486baefab2996b01f97a.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:44 async_llm_engine.py:524] Received request cmpl-095f8488279c40eea8c6dfafedd31072: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:44 async_llm_engine.py:120] Finished request cmpl-095f8488279c40eea8c6dfafedd31072.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:44 async_llm_engine.py:524] Received request cmpl-5c605da83cf9474cb5d832fd3d002c1b: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:45 async_llm_engine.py:120] Finished request cmpl-5c605da83cf9474cb5d832fd3d002c1b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:45 async_llm_engine.py:524] Received request cmpl-fd1266f4d29c4ad38b9100b5c585639f: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“2020年4月16号三亚下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 17, 15, 17, 15, 7948, 19, 9754, 16, 21, 17992, 107442, 111926, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:45 async_llm_engine.py:120] Finished request cmpl-fd1266f4d29c4ad38b9100b5c585639f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:45 async_llm_engine.py:524] Received request cmpl-3e83dcb0d0084eb494214c73f4ccf3b2: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛3-15号天气预报”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8110, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 18, 12, 16, 20, 17992, 104307, 115744, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:45 async_llm_engine.py:120] Finished request cmpl-3e83dcb0d0084eb494214c73f4ccf3b2.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:45 async_llm_engine.py:524] Received request cmpl-4ff9bbcb53ef489faaf3f99dfa161683: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“5月6号下雪么, 城市是威海”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 20, 9754, 21, 17992, 16872, 100167, 81596, 11, 4891, 253, 236, 22697, 20412, 110582, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:46 async_llm_engine.py:120] Finished request cmpl-4ff9bbcb53ef489faaf3f99dfa161683.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:46 async_llm_engine.py:524] Received request cmpl-72fcde43dcca40bd9b480f28f590d6cd: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“青岛2023年12月30号有雾霾么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8102, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 102240, 17, 15, 17, 18, 7948, 16, 17, 9754, 18, 15, 17992, 18830, 116309, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:46 async_llm_engine.py:120] Finished request cmpl-72fcde43dcca40bd9b480f28f590d6cd.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:46 async_llm_engine.py:524] Received request cmpl-17ce96c4c017460d88745fa342de4824: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“我打算6月1号去北京旅游，请问天气怎么样？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 35946, 104868, 21, 9754, 16, 17992, 85336, 68990, 99790, 37945, 56007, 104307, 104472, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:46 async_llm_engine.py:120] Finished request cmpl-17ce96c4c017460d88745fa342de4824.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:46 async_llm_engine.py:524] Received request cmpl-abf1a2fcfbdd4e18b9442d8b3b7af0b8: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“你们打算1月3号坐哪一趟航班去上海？”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8105, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 101900, 104868, 16, 9754, 18, 17992, 99901, 99459, 111270, 107501, 85336, 100633, 11319, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:46 async_llm_engine.py:120] Finished request cmpl-abf1a2fcfbdd4e18b9442d8b3b7af0b8.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:46 async_llm_engine.py:524] Received request cmpl-3641657cb8b841a48541c9e079355447: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“小明和小红是8月8号在上海结婚么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8104, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 30709, 30858, 33108, 30709, 99425, 20412, 23, 9754, 23, 17992, 106607, 104388, 81596, 12390, 3837, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:47 async_llm_engine.py:120] Finished request cmpl-3641657cb8b841a48541c9e079355447.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:47 async_llm_engine.py:524] Received request cmpl-ab0bbafaf47445d2b7f5ee495845cb0f: prompt: '<|im_start|>system\\n你是一个人工智能助手.<|im_end|>\\n<|im_start|>user\\n\\n给定一句话：“一起去东北看冰雕么, 大概是1月15号左右, 我们3个人一起”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n<|im_end|>\\n<|im_start|>assistant\\n', sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8094, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [151644, 8948, 198, 56568, 101909, 104455, 110498, 13, 151645, 198, 151644, 872, 271, 89012, 22382, 105321, 36987, 110926, 104756, 50930, 100038, 100903, 81596, 11, 40666, 100, 99706, 20412, 16, 9754, 16, 20, 17992, 101081, 11, 49434, 239, 79478, 18, 99605, 100018, 33590, 112720, 59879, 105652, 101882, 99257, 3407, 105652, 16, 5122, 102450, 106599, 101047, 99490, 33108, 45785, 54899, 17, 18947, 27369, 198, 105652, 17, 5122, 100345, 99490, 33108, 45785, 27369, 3837, 43959, 5370, 66558, 3837, 68805, 17714, 4913, 8926, 788, 99490, 1335, 1028, 788, 45785, 630, 109194, 3837, 99487, 5370, 66558, 20412, 28311, 151645, 198, 151644, 77091, 198], lora_request: None.\n",
      "INFO 05-16 13:56:47 async_llm_engine.py:120] Finished request cmpl-ab0bbafaf47445d2b7f5ee495845cb0f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:38156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-16 13:56:54 metrics.py:229] Avg prompt throughput: 120.7 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:57:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:57:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:57:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:57:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:57:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:57:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:58:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:58:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:58:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:58:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:58:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:58:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:59:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:59:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:59:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:59:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:59:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 13:59:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:00:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:00:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:00:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:00:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:00:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:00:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:01:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:01:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:01:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:01:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:01:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:01:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:02:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:02:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:02:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:02:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:02:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:02:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:03:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:03:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:03:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:03:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:03:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:03:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:04:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:04:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:04:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:04:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:04:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:04:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:05:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:05:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:05:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:05:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:05:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:05:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:06:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:06:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:06:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:06:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:06:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:06:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:07:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:07:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:07:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:07:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:07:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:07:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:08:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:08:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:08:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:08:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:08:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:08:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:09:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:09:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:09:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:09:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:09:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:09:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:10:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:10:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:10:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:10:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:10:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:10:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:11:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:11:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:11:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:11:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:11:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:11:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:12:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:12:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:12:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:12:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:12:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:12:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:13:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:13:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:13:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:13:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:13:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:13:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:14:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:14:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:14:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:14:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:14:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:14:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:15:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:15:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:15:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:15:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:15:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:15:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:16:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:16:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:16:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:16:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:16:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:16:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:17:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:17:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:17:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:17:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:17:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:17:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:18:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:18:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:18:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:18:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:18:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:18:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:19:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:19:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:19:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:19:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:19:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:19:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:20:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:20:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:20:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:20:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:20:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:20:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:21:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:21:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:21:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:21:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:21:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:21:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:22:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:22:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:22:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:22:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:22:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:22:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:23:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:23:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:23:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:23:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:23:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:23:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:24:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:24:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:24:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:24:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:24:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:24:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:25:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:25:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:25:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:25:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:25:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:25:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:26:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:26:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:26:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:26:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:26:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:26:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:27:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:27:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:27:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:27:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:27:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:27:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:28:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:28:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:28:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:28:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:28:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:28:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:29:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:29:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:29:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:29:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:29:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:29:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:30:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:30:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:30:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:30:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:30:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:30:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:31:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:31:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:31:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:31:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:31:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:31:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:32:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:32:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:32:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:32:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:32:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:32:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:33:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:33:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:33:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:33:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:33:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:33:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:34:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:34:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:34:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:34:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:34:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:34:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:35:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:35:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:35:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:35:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:35:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:35:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:36:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:36:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:36:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:36:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:36:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:36:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:37:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:37:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:37:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:37:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:37:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:37:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:38:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:38:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:38:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:38:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:38:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:38:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:39:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:39:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:39:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:39:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:39:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:39:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:40:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:40:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:40:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:40:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:40:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:40:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:41:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:41:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:41:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:41:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:41:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:41:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:42:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:42:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:42:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:42:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:42:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:42:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:43:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:43:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:43:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:43:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:43:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:43:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:44:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:44:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:44:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:44:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:44:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:44:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:45:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:45:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:45:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:45:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:45:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:45:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:46:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:46:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:46:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:46:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:46:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:46:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:47:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:47:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:47:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:47:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:47:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:47:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:48:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:48:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:48:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:48:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:48:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:48:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:49:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:49:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:49:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:49:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:49:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:49:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:50:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:50:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:50:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:50:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:50:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:50:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:51:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:51:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:51:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:51:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:51:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:51:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:52:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:52:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:52:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:52:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:52:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:52:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:53:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:53:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:53:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:53:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:53:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:53:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:54:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:54:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:54:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:54:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:54:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:54:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:55:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:55:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:55:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:55:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:55:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:55:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:56:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:56:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:56:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:56:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:56:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:56:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:57:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:57:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:57:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:57:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:57:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:57:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:58:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:58:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:58:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:58:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:58:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:58:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:59:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:59:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:59:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:59:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:59:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 14:59:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:00:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:00:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:00:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:00:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:00:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:00:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:01:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:01:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:01:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:01:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:01:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:01:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:02:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:02:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:02:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:02:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:02:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:02:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:03:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:03:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:03:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:03:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:03:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:03:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:04:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:04:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:04:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:04:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:04:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:04:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:05:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:05:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:05:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:05:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:05:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:05:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:06:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:06:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:06:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:06:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:06:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:06:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:07:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:07:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:07:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:07:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:07:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:07:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:08:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:08:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:08:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:08:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:08:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:08:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:09:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:09:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:09:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:09:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:09:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:09:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:10:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:10:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:10:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:10:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:10:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:10:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:11:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:11:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:11:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:11:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:11:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:11:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:12:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:12:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:12:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:12:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:12:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:12:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:13:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:13:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:13:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:13:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:13:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:13:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:14:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:14:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:14:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:14:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:14:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:14:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:15:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:15:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:15:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:15:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:15:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:15:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:16:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:16:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:16:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:16:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:16:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:16:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:17:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:17:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:17:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:17:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:17:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:17:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:18:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:18:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:18:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:18:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:18:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:18:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:19:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:19:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:19:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:19:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:19:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:19:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:20:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:20:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:20:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:20:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:20:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:20:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:21:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:21:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:21:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:21:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:21:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:21:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:22:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:22:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:22:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:22:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:22:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:22:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:23:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:23:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:23:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:23:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:23:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:23:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:24:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:24:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:24:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:24:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:24:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:24:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:25:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:25:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:25:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:25:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:25:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:25:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:26:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:26:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:26:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:26:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:26:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:26:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:27:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:27:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:27:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:27:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:27:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:27:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:28:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:28:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:28:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:28:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:28:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:28:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:29:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:29:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:29:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:29:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:29:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:29:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:30:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:30:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:30:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:30:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:30:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:30:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:31:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:31:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:31:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:31:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:31:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:31:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:32:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:32:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:32:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:32:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:32:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:32:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:33:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:33:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:33:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:33:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:33:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:33:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:34:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:34:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:34:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:34:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:34:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:34:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:35:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:35:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:35:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:35:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:35:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:35:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:36:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:36:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:36:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:36:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:36:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:36:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:37:04 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:37:14 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:37:24 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:37:34 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:37:44 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-16 15:37:54 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m2908491\u001b[0m]\n",
      "Exception in callback functools.partial(<function _raise_exception_on_finish at 0x7f2706923060>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f26fc0bba10>>)\n",
      "handle: <Handle functools.partial(<function _raise_exception_on_finish at 0x7f2706923060>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f26fc0bba10>>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\n",
      "    task.result()\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 490, in run_engine_loop\n",
      "    await self._request_tracker.wait_for_new_requests()\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 189, in wait_for_new_requests\n",
      "    await self.new_requests_event.wait()\n",
      "  File \"/data/liucd/anaconda/lib/python3.11/asyncio/locks.py\", line 213, in wait\n",
      "    await fut\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "# # 量化版本\n",
    "!python -m vllm.entrypoints.openai.api_server --served-model-name quant_lora \\\n",
    "      --model merge_lora   --port 1122  # --quantization awq \\\n",
    "      --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --trust-remote-code  \\\n",
    "      # --dtype  float16\n",
    "\n",
    "# # 非量化版本\n",
    "# !python -m vllm.entrypoints.openai.api_server --served-model-name quant_lora \\\n",
    "#       --model /data/liucd/langchain/Qwen/Qwen1___5-7B-Chat-AWQ/  --port 1122 \\\n",
    "#       --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --trust-remote-code --quantization awq\n",
    "\n",
    "# ! python -m vllm.entrypoints.api_server  \\\n",
    "#     --model /data/liucd/langchain/Qwen/Qwen1___5-7B-Chat/  --port 1122 \\\n",
    "#       --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --trust-remote-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e7b6781-fd4a-4eba-8763-b129346d7961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 12:20:33 api_server.py:149] vLLM API server version 0.4.0.post1\n",
      "INFO 05-15 12:20:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=1122, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name='Qwen-7B-Lora_Chat', lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/data/liucd/langchain/Qwen/Qwen-7B-Chat', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='float16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=1, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=24, gpu_memory_utilization=0.9, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=True, max_log_len=None)\n",
      "INFO 05-15 12:20:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='/data/liucd/langchain/Qwen/Qwen-7B-Chat', tokenizer='/data/liucd/langchain/Qwen/Qwen-7B-Chat', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "WARNING 05-15 12:20:34 tokenizer.py:104] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 05-15 12:20:35 selector.py:16] Using FlashAttention backend.\n",
      "INFO 05-15 12:20:44 model_runner.py:104] Loading model weights took 14.3860 GB\n",
      "INFO 05-15 12:20:47 gpu_executor.py:94] # GPU blocks: 2480, # CPU blocks: 3072\n",
      "WARNING 05-15 12:21:32 tokenizer.py:104] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "WARNING 05-15 12:21:32 serving_chat.py:335] No chat template provided. Chat API will not work.\n",
      "WARNING 05-15 12:21:32 tokenizer.py:104] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1731548\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:1122\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 05-15 12:21:42 metrics.py:218] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-15 12:21:52 metrics.py:218] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-15 12:22:02 metrics.py:218] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "INFO 05-15 12:22:12 metrics.py:218] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 05-15 12:22:17 metrics.py:218] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 46.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for connections to close. (CTRL+C to force quit)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0, 5'\n",
    "\n",
    "# 非量化版本: 7B 模型\n",
    "# !python -m vllm.entrypoints.openai.api_server --served-model-name Qwen-7B-Lora_Chat \\\n",
    "#       --model /data/liucd/langchain/Qwen/Qwen-7B-Chat/  --port 1122 --trust-remote-code \\\n",
    "#         --gpu-memory-utilization 0.8\n",
    "\n",
    "! python -m vllm.entrypoints.openai.api_server  --served-model-name Qwen-7B-Lora_Chat \\\n",
    "    --model  /data/liucd/langchain/Qwen/Qwen-7B-Chat  --swap-space 24 \\\n",
    "    --disable-log-requests --trust-remote-code --max-num-seqs 256 --host 0.0.0.0 --port 1122 \\\n",
    "    --dtype float16 --max-parallel-loading-workers 1  --enforce-eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46159302-0946-4e34-99af-2eda83da6ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[1;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_sync/connection.py:122\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 122\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    123\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_backends/sync.py:205\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    203\u001b[0m }\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    206\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    207\u001b[0m         address,\n\u001b[1;32m    208\u001b[0m         timeout,\n\u001b[1;32m    209\u001b[0m         source_address\u001b[38;5;241m=\u001b[39msource_address,\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:898\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 898\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    899\u001b[0m         request,\n\u001b[1;32m    900\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m    901\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    902\u001b[0m     )\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:232\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/httpx/_transports/default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m openai_api_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:1122/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      7\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_api_key,\n\u001b[1;32m      8\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mopenai_api_base,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen1.5-7B-Chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     14\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你是一个人工智能助手.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     15\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m告诉我有关北京的特产\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     16\u001b[0m     ]\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_response)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    661\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    662\u001b[0m             {\n\u001b[1;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    683\u001b[0m             },\n\u001b[1;32m    684\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    685\u001b[0m         ),\n\u001b[1;32m    686\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    687\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    688\u001b[0m         ),\n\u001b[1;32m    689\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    690\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    692\u001b[0m     )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:1180\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1177\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1178\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1179\u001b[0m     )\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:869\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    862\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    870\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    871\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    872\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    873\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    874\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    875\u001b[0m     )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    923\u001b[0m         options,\n\u001b[1;32m    924\u001b[0m         cast_to,\n\u001b[1;32m    925\u001b[0m         retries,\n\u001b[1;32m    926\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    927\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    928\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    994\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    995\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    996\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m    997\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    998\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    999\u001b[0m )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    923\u001b[0m         options,\n\u001b[1;32m    924\u001b[0m         cast_to,\n\u001b[1;32m    925\u001b[0m         retries,\n\u001b[1;32m    926\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    927\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    928\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    994\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    995\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    996\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m    997\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    998\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    999\u001b[0m )\n",
      "File \u001b[0;32m/data/liucd/anaconda/lib/python3.11/site-packages/openai/_base_client.py:932\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    923\u001b[0m             options,\n\u001b[1;32m    924\u001b[0m             cast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m             response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    929\u001b[0m         )\n\u001b[1;32m    931\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 932\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    934\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl, response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mreason_phrase\n\u001b[1;32m    936\u001b[0m )\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:1122/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"Qwen1.5-7B-Chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个人工智能助手.\"},\n",
    "        {\"role\": \"user\", \"content\": \"告诉我有关北京的特产\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82b5cb-c305-4537-867b-045b33110486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
